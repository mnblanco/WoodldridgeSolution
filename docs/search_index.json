[
["index.html", "Introductory Econometrics: A Modern Approach Solutions Welcome", " Introductory Econometrics: A Modern Approach Solutions Marjorie Blanco 2019-01-02 Welcome This contains solutions to the exercise in Introductory Econometrics: A Modern Approach by Woodldridge. This work is licensed under a Creative Commons Attribution 4.0 International License "],
["nature-of-econometrics-and-economic-data.html", "Chapter 1 Nature of econometrics and economic Data", " Chapter 1 Nature of econometrics and economic Data "],
["the-simple-regression-model.html", "Chapter 2 The simple regression Model 2.1 C1-K401K 2.2 C2-CEOSAL2 2.3 C3-SLEEP75 2.4 C4-WAGE2 2.5 C5-RDCHEM 2.6 C6-MEAP93 2.7 C7-CHARITU 2.8 C8 2.9 C9-countymurders 2.10 C10-CATHOLIC", " Chapter 2 The simple regression Model 2.1 C1-K401K The data in 401K are a subset of data analyzed by Papke (1995) to study the relationship between participation in a 401(k) pension plan and the generosity of the plan. The variable prate is the percentage of eligible workers with an active account; this is the variable we would like to explain. The measure of generosity is the plan match rate, mrate. This variable gives the average amount the firm contributes to each worker’s plan for each $1 contribution by the worker. For example, if mrate 5 0.50, then a $1 contribution by the worker is matched by a 50¢ contribution by the firm. A data.frame with 1534 observations on 8 variables: prate: participation rate, percent mrate: 401k plan match rate totpart: total 401k participants totelg: total eligible for 401k plan age: age of 401k plan totemp: total number of firm employees sole: = 1 if 401k is firm’s sole plan ltotemp: log of totemp Find the average participation rate (prate) and the average match rate (mrate) in the sample of plans. Avg prate : 87.3629075 Avg mrate 0.7315124 Now, estimate the simple regression equation \\[\\widehat{prate} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 * mrate\\] and report the results along with the sample size and R-squared. ## ## Call: ## lm(formula = prate ~ mrate, data = k401k) ## ## Residuals: ## Min 1Q Median 3Q Max ## -82.303 -8.184 5.178 12.712 16.807 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 83.0755 0.5633 147.48 &lt;2e-16 *** ## mrate 5.8611 0.5270 11.12 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 16.09 on 1532 degrees of freedom ## Multiple R-squared: 0.0747, Adjusted R-squared: 0.0741 ## F-statistic: 123.7 on 1 and 1532 DF, p-value: &lt; 2.2e-16 Sample size is 1534. \\(R^{2}\\) value is 0.0747. Adjusted \\(R^{2}\\) value is 0.0741. Interpret the intercept in your equation. Interpret the coefficient on mrate. An increase in mrate is associated with a 5.86 increase in the average prate and this effect is is statistically significant at 1%. The intercept implies that, even if mrate = 0, the predicted participation rate is 83.1 percent. The coefficient on mrate implies that a one-dollar increase in the match rate – a fairly large increase – is estimated to increase prate by 5.86 percentage points. This assumes, of course, that this change prate is possible (if, say, prate is already at 98, this interpretation makes no sense). Find the predicted prate when mrate 3.5. Is this a reasonable prediction? Explain what is happening here. ## $fit ## 1 ## 103.5892 ## ## $se.fit ## [1] 1.515723 ## ## $df ## [1] 1532 ## ## $residual.scale ## [1] 16.08528 ## [1] 103.5892 This amount is impossible, as we can have at most a 100 percent participation rate. This illustrates that, especially when dependent variables are bounded, a simple regression model can give strange predictions for extreme values of the independent variable. In the sample of 1,534 firms, only 34 have mrate = 3.5. How much of the variation in prate is explained by mrate? Is this a lot in your opinion? The Adjusted \\(R^{2}\\) value is 0.0741. This tells us that 7.41% of the variation in percentage of eligible workers with an active account, as quantified by prate, is explained by mrate. mrate explains about 7.5% of the variation in prate. This is not much, and suggests that many other factors influence 401(k) plan participation rates. 2.2 C2-CEOSAL2 The data set in CEOSAL2 contains information on chief executive officers for U.S. corporations. The variable salary is annual compensation, in thousands of dollars, and ceoten is prior number of years as company CEO. A data.frame with 177 observations on 15 variables: salary: 1990 compensation, $1000s age: in years college: =1 if attended college grad: =1 if attended graduate school comten: years with company ceoten: years as ceo with company sales: 1990 firm sales, millions profits: 1990 profits, millions mktval: market value, end 1990, mills. lsalary: log(salary) lsales: log(sales) lmktval: log(mktval) comtensq: comten^2 ceotensq: ceoten^2 profmarg: profits as percent of sales Find the average salary and the average tenure in the sample. Average salary (1990 compensation, $1000s) ## [1] 865.8644 Average salary (log) ## [1] 6.582848 Average tenure ## [1] 7.954802 How many CEOs are in their first year as CEO (that is, ceoten 5 0)? What is the longest tenure as a CEO? Number of CEOs in their first year as CEO ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 5 Longest tenure as a CEO ## [1] 37 Estimate the simple regression model \\[log(salary) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 * ceoten + u\\] and report your results in the usual form. What is the (approximate) predicted percentage increase in salary given one more year as a CEO? Sample size is 177. ## ## Call: ## lm(formula = lsalary ~ ceoten, data = ceosal2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.15314 -0.38319 -0.02251 0.44439 1.94337 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.505498 0.067991 95.682 &lt;2e-16 *** ## ceoten 0.009724 0.006364 1.528 0.128 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6038 on 175 degrees of freedom ## Multiple R-squared: 0.01316, Adjusted R-squared: 0.007523 ## F-statistic: 2.334 on 1 and 175 DF, p-value: 0.1284 2.3 C3-SLEEP75 Use the data in SLEEP75 from Biddle and Hamermesh (1990) to study whether there is a tradeoff between the time spent sleeping per week and the time spent in paid work. We could use either variable as the dependent variable. For concreteness, estimate the model \\[sleep = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 * totwrk + u\\] where sleep is minutes spent sleeping at night per week and totwrk is total minutes worked during the week. A data.frame with 706 observations on 34 variables: age: in years black: =1 if black case: identifier clerical: =1 if clerical worker construc: =1 if construction worker educ: years of schooling earns74: total earnings, 1974 gdhlth: =1 if in good or excel. health inlf: =1 if in labor force leis1: sleep - totwrk leis2: slpnaps - totwrk leis3: rlxall - totwrk smsa: =1 if live in smsa lhrwage: log hourly wage lothinc: log othinc, unless othinc &lt; 0 male: =1 if male marr: =1 if married prot: =1 if Protestant rlxall: slpnaps + personal activs selfe: =1 if self employed sleep: mins sleep at night, per wk slpnaps: minutes sleep, inc. naps south: =1 if live in south spsepay: spousal wage income spwrk75: =1 if spouse works totwrk: mins worked per week union: =1 if belong to union worknrm: mins work main job workscnd: mins work second job exper: age - educ - 6 yngkid: =1 if children &lt; 3 present yrsmarr: years married hrwage: hourly wage agesq: age^2 ## ## Call: ## lm(formula = sleep ~ totwrk, data = sleep75) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2429.94 -240.25 4.91 250.53 1339.72 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3586.37695 38.91243 92.165 &lt;2e-16 *** ## totwrk -0.15075 0.01674 -9.005 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 421.1 on 704 degrees of freedom ## Multiple R-squared: 0.1033, Adjusted R-squared: 0.102 ## F-statistic: 81.09 on 1 and 704 DF, p-value: &lt; 2.2e-16 Report your results in equation form along with the number of observations and \\(R^{2}\\). What does the intercept in this equation mean? \\[sleep = 3586.4 - 0.151 * totwrk\\] Sample size is 706. \\(R^{2}\\) value is 0.1033. Adjusted \\(R^{2}\\) value is 0.102. The intercept implies that the estimated amount of sleep per week (sleep) for someone who does not work (totwrk = 0) is 3590 minutes, or about 59.8 hours. This comes to about 8.5 hours per night. If totwrk increases by 2 hours, by how much is sleep estimated to fall? Do you find this to be a large effect? If someone works two more hours per week then totwrk = 120 (totwrk is measured in minutes not hours). The amount of sleep would decrease by -0.151(120) = -18.1 minutes. ## $fit ## 1 ## 3568.287 ## ## $se.fit ## [1] 37.08682 ## ## $df ## [1] 704 ## ## $residual.scale ## [1] 421.1357 ## [1] -18.0895 2.4 C4-WAGE2 Use the data in WAGE2 to estimate a simple regression explaining monthly salary (wage) in terms of IQ score (IQ). A data.frame with 935 observations on 17 variables: wage: monthly earnings hours: average weekly hours IQ: IQ score KWW: knowledge of world work score educ: years of education exper: years of work experience tenure: years with current employer age: age in years married: =1 if married black: =1 if black south: =1 if live in south urban: =1 if live in SMSA sibs: number of siblings brthord: birth order meduc: mother’s education feduc: father’s education lwage: natural log of wage Find the average salary and average IQ in the sample. What is the sample standard deviation of IQ? (IQ scores are standardized so that the average in the population is 100 with a standard deviation equal to 15.) ## [1] 957.9455 ## [1] 101.2824 ## [1] 15.05264 Estimate a simple regression model where a one-point increase in IQ changes wage by a constant dollar amount. Use this model to find the predicted increase in wage for an increase in IQ of 15 points. Does IQ explain most of the variation in wage? ## ## Call: ## lm(formula = wage ~ IQ, data = wage2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -898.7 -256.5 -47.3 201.1 2072.6 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 116.9916 85.6415 1.366 0.172 ## IQ 8.3031 0.8364 9.927 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 384.8 on 933 degrees of freedom ## Multiple R-squared: 0.09554, Adjusted R-squared: 0.09457 ## F-statistic: 98.55 on 1 and 933 DF, p-value: &lt; 2.2e-16 Sample size is 935. \\(R^{2}\\) value is 0.0955. Adjusted \\(R^{2}\\) value is 0.0946. Estimate a simple regression model where a one-point increase in IQ changes wage by a constant dollar amount. Use this model to find the predicted increase in wage for an increase in IQ of 15 points. Does IQ explain most of the variation in wage? ## $fit ## 1 ## 241.5375 ## ## $se.fit ## [1] 73.25496 ## ## $df ## [1] 933 ## ## $residual.scale ## [1] 384.7667 Now, estimate a model where each one-point increase in IQ has the same percent- age effect on wage. If IQ increases by 15 points, what is the approximate percent- age increase in predicted wage? 2.5 C5-RDCHEM For the population of firms in the chemical industry, let rd denote annual expenditures on research and development, and let sales denote annual sales (both are in millions of dollars). A data.frame with 32 observations on 8 variables: rd: R&amp;D spending, millions sales: firm sales, millions profits: profits, millions rdintens: rd as percent of sales profmarg: profits as percent of sales salessq: sales^2 lsales: log(sales) lrd: log(rd) Write down a model (not an estimated equation) that implies a constant elasticity between rd and sales. Which parameter is the elasticity? The constant elasticity model is a log-log model: \\[ log(rd) = \\beta_0 + \\beta_1 * log(sales) \\] where \\(\\beta_1\\) is the elasticity of rd with respect to sales. ## ## Call: ## lm(formula = lrd ~ lsales, data = rdchem) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.90406 -0.40086 -0.02178 0.40562 1.10439 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.10472 0.45277 -9.066 4.27e-10 *** ## lsales 1.07573 0.06183 17.399 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5294 on 30 degrees of freedom ## Multiple R-squared: 0.9098, Adjusted R-squared: 0.9068 ## F-statistic: 302.7 on 1 and 30 DF, p-value: &lt; 2.2e-16 Now, estimate the model using the data in RDCHEM.RAW. Write out the estimated equation in the usual form. What is the estimated elasticity of rd with respect to sales? Explain in words what this elasticity means. ## ## Call: ## lm(formula = lrd ~ lsales, data = rdchem) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.90406 -0.40086 -0.02178 0.40562 1.10439 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.10472 0.45277 -9.066 4.27e-10 *** ## lsales 1.07573 0.06183 17.399 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5294 on 30 degrees of freedom ## Multiple R-squared: 0.9098, Adjusted R-squared: 0.9068 ## F-statistic: 302.7 on 1 and 30 DF, p-value: &lt; 2.2e-16 The estimated equation is: \\[ log(rd) = -4.105 + 1.076 * log(sales) \\] The estimated elasticity of rd with respect to sales is 1.08, which is just above one. A one percent increase in sales is estimated to increase rd by about 1.1%. 2.6 C6-MEAP93 We used the data in MEAP93 for Example 2.12. Now we want to explore the relationship between the math pass rate (math10) and spending per student (expend). A data.frame with 408 observations on 17 variables: lnchprg: perc of studs in sch lnch prog enroll: school enrollment staff: staff per 1000 students expend: expend. per stud, $ salary: avg. teacher salary, $ benefits: avg. teacher benefits, $ droprate: school dropout rate, perc gradrate: school graduation rate, perc math10: perc studs passing MEAP math sci11: perc studs passing MEAP science totcomp: salary + benefits ltotcomp: log(totcomp) lexpend: log of expend lenroll: log(enroll) lstaff: log(staff) bensal: benefits/salary lsalary: log(salary) Do you think each additional dollar spent has the same effect on the pass rate, or does a diminishing effect seem more appropriate? Explain. Additional dollar spent does not have same effect on the pass rate. I expect diminishing effect. ## ## Call: ## lm(formula = math10 ~ expend, data = meap93) ## ## Residuals: ## Min 1Q Median 3Q Max ## -22.579 -7.175 -0.874 6.299 39.174 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.336e+01 2.934e+00 4.553 7e-06 *** ## expend 2.456e-03 6.601e-04 3.720 0.000227 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.33 on 406 degrees of freedom ## Multiple R-squared: 0.03296, Adjusted R-squared: 0.03058 ## F-statistic: 13.84 on 1 and 406 DF, p-value: 0.0002273 In the population model \\[math10 + \\widehat{\\beta}_0 + \\widehat{\\beta}_1 log(expend ) + u\\] argue that b1/10 is the percentage point change in math10 given a 10% increase in expend. \\[\\beta_1 /10\\] is the percentage point change in math10 given a 10% increase in expend. ## ## Call: ## lm(formula = math10 ~ lexpend, data = meap93) ## ## Residuals: ## Min 1Q Median 3Q Max ## -22.343 -7.100 -0.914 6.148 39.093 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -69.341 26.530 -2.614 0.009290 ** ## lexpend 11.164 3.169 3.523 0.000475 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.35 on 406 degrees of freedom ## Multiple R-squared: 0.02966, Adjusted R-squared: 0.02727 ## F-statistic: 12.41 on 1 and 406 DF, p-value: 0.0004752 Use the data in MEAP93 to estimate the model from part (ii). Report the estimated equation in the usual way, including the sample size and R-squared. A 10% increase in expendidure is associated with a 11.2 increase in the average perc studs passing MEAP math and this effect is is statistically significant at 1%. How big is the estimated spending effect? Namely, if spending increases by 10%, what is the estimated percentage point increase in math10? One might worry that regression analysis can produce fitted values for math10 that are greater than 100. Why is this not much of a worry in this data set? 2.7 C7-CHARITU Use the data in CHARITY [obtained from Franses and Paap (2001)] to answer the following questions: A data.frame with 4268 observations on 8 variables: respond: =1 if responded with gift gift: amount of gift, Dutch guilders resplast: =1 if responded to most recent mailing weekslast: number of weeks since last response propresp: response rate to mailings mailsyear: number of mailings per year giftlast: amount of most recent gift avggift: average of past gifts What is the average gift in the sample of 4,268 people (in Dutch guilders)? What percentage of people gave no gift? Average gift (Dutch guilders) and % of people that gave no gift ## [1] 7.44447 ## [1] 0.6000469 The average gift is about 7.4444705 Dutch guilders. Out of 4268 respondents, 2561 did not give a gift, or about 60 percent. What is the average mailings per year? What are the minimum and maximum values? Average, min and max mailing per year ## [1] 2.049555 ## [1] 0.25 ## [1] 3.5 The average mailings per year is about 2.05. The minimum value is 0.25 (which presumably means that someone has been on the mailing list for at least four years) and the maximum value is 3.5. Estimate the model \\[gift = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 * mailsyear + u\\] by OLS and report the results in the usual way, including the sample size and R-squared. ## ## Call: ## lm(formula = gift ~ mailsyear, data = charity) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.287 -7.976 -5.976 2.687 245.999 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.0141 0.7395 2.724 0.00648 ** ## mailsyear 2.6495 0.3431 7.723 1.4e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.96 on 4266 degrees of freedom ## Multiple R-squared: 0.01379, Adjusted R-squared: 0.01356 ## F-statistic: 59.65 on 1 and 4266 DF, p-value: 1.404e-14 Sample size is 4268. \\(R^{2}\\) value is 0.0138. Adjusted \\(R^{2}\\) value is 0.0136. Interpret the slope coefficient. If each mailing costs one guilder, is the charity expected to make a net gain on each mailing? Does this mean the charity makes a net gain on every mailing? Explain. The estimated equation is \\[gift = 2.0141 + 2.6495 * mailsyear\\] The slope coefficient means that each mailing per year is associated with an estimated 2.65 additional guilders, on average. Therefore, if each mailing costs one guilder, the expected profit from each mailing is estimated to be 2.65 guilders. This is only the average, however. What is the smallest predicted charitable contribution in the sample? Using this simple regression analysis, can you ever predict zero for gift? Min gift where some people have received 0.25 mailings: 0 ## $fit ## 1 ## 2.01408 ## ## $se.fit ## [1] 0.7394696 ## ## $df ## [1] 4266 ## ## $residual.scale ## [1] 14.9601 The smallest mailsyear in the sample is 0.25. The smallest predicted value of gifts is 2.01 + 2.65 (.25) = 2.68. With this estimated equation, we never predict zero charitable gifts. 2.8 C8 To complete this exercise you need a software package that allows you to generate data from the uniform and normal distributions. Start by generating 500 observations xi – the explanatory variable – from the uniform distribution with range [0,10]. (Most statistical packages have a command for the Uniform[0,1] distribution; just multiply those observations by 10.) What are the sample mean and sample standard deviation of the xi? The expected mean is about 5. ## [1] 5.087806 ## [1] 2.906681 Randomly generate 500 errors, ui, from the Normal[0,36] distribution. (If you generate a Normal[0,1], as is commonly available, simply multiply the outcomes by six.) Is the sample average of the ui exactly zero? Why or why not? What is the sample standard deviation of the ui? The expected mean is about 18. ## [1] 19.09546 ## [1] 10.29605 Now generate the yi as \\[y_i = 1 + 2x_i + u_i = \\widehat{\\beta}_0 + \\beta x_i + ui\\]; that is, the population intercept is one and the population slope is two. Use the data to run the regression of yi on xi. What are your estimates of the intercept and slope? Are they equal to the population values in the above equation? Explain. Obtain the OLS residuals, uˆi, and verify that equation (2.60) hold (subject to rounding error). Compute the same quantities in equation (2.60) but use the errors ui in place of the residuals. Now what do you conclude? Repeat parts (i), (ii), and (iii) with a new sample of data, starting with generating the xi. Now what do you obtain for bˆ0 and bˆ1? Why are these different from what you obtained in part (iii)? ## ## Call: ## lm(formula = y ~ x, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.4113 -8.8657 0.5281 8.7555 17.2243 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 19.2886 0.9289 20.76 &lt;2e-16 *** ## x 2.1586 0.1586 13.61 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.3 on 498 degrees of freedom ## Multiple R-squared: 0.2712, Adjusted R-squared: 0.2697 ## F-statistic: 185.3 on 1 and 498 DF, p-value: &lt; 2.2e-16 ## ## Call: ## lm(formula = y1 ~ x, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.4113 -8.8657 0.5281 8.7555 17.2243 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 18.2886 0.9289 19.688 &lt; 2e-16 *** ## x 1.1586 0.1586 7.306 1.1e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.3 on 498 degrees of freedom ## Multiple R-squared: 0.09682, Adjusted R-squared: 0.095 ## F-statistic: 53.38 on 1 and 498 DF, p-value: 1.098e-12 2.9 C9-countymurders A data.frame with 37349 observations on 20 variables: arrests: # of murder arrests countyid: county identifier: 1000*statefips + countyfips density: population density; per square mile popul: county population perc1019: percent pop. age 10-19 perc2029: percent pop. age 20-29 percblack: percent population black percmale: percent population male rpcincmaint: real per capita income maintenance rpcpersinc: real per capita personal income rpcunemins: real per capita unem insurance payments year: 1980-1996 murders: # of murders murdrate: murders per 10,000 people arrestrate: murder arrests per 10,000 statefips: state FIPS code countyfips: county FIPS code execs: # of executions lpopul: log(popul) execrate: executions per 10,000 ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 1051 ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 31 ## max_execs ## 1 3 ## ## Call: ## lm(formula = murders ~ execs, data = countymurders %&gt;% filter(year == ## 1996)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -149.12 -5.46 -4.46 -2.46 1338.99 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.4572 0.8348 6.537 7.79e-11 *** ## execs 58.5555 5.8333 10.038 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 38.89 on 2195 degrees of freedom ## Multiple R-squared: 0.04389, Adjusted R-squared: 0.04346 ## F-statistic: 100.8 on 1 and 2195 DF, p-value: &lt; 2.2e-16 ## $fit ## 1 ## 5.457241 ## ## $se.fit ## [1] 0.834838 ## ## $df ## [1] 2195 ## ## $residual.scale ## [1] 38.88748 ## [1] 5.457241 2.10 C10-CATHOLIC A data.frame with 7430 observations on 13 variables: id: person identifier read12: reading standardized score math12: mathematics standardized score female: =1 if female asian: =1 if Asian hispan: =1 if Hispanic black: =1 if black motheduc: mother’s years of education fatheduc: father’s years of education lfaminc: log of family income hsgrad: =1 if graduated from high school by 1994 cathhs: =1 if attended Catholic HS parcath: =1 if a parent reports being Catholic Sample size ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 7430 Means and standard deviation for math12 and read12 ## math12_mean math12_sd read12_mean read12_sd ## 1 52.13362 9.459117 51.7724 9.407761 ## ## Call: ## lm(formula = math12 ~ read12, data = catholic) ## ## Residuals: ## Min 1Q Median 3Q Max ## -24.5477 -4.5934 0.1838 4.6984 27.0182 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 15.15304 0.43204 35.07 &lt;2e-16 *** ## read12 0.71429 0.00821 87.00 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.658 on 7428 degrees of freedom ## Multiple R-squared: 0.5047, Adjusted R-squared: 0.5046 ## F-statistic: 7569 on 1 and 7428 DF, p-value: &lt; 2.2e-16 An increase in reading standardized score is associated with a 0.714 increase in the average mathematics standardized score and this effect is is statistically significant at 1%. ## ## Call: ## lm(formula = read12 ~ math12, data = catholic) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.1459 -4.2021 0.4885 4.4920 22.8935 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.937062 0.430318 34.71 &lt;2e-16 *** ## math12 0.706556 0.008122 87.00 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.621 on 7428 degrees of freedom ## Multiple R-squared: 0.5047, Adjusted R-squared: 0.5046 ## F-statistic: 7569 on 1 and 7428 DF, p-value: &lt; 2.2e-16 "],
["multiple-regression-analysis-estimation.html", "Chapter 3 Multiple regression Analysis: estimation 3.1 Formula 3.2 Explore 3.3 Example 3.1 3.4 Example 3.2 3.5 Example 3.3 3.6 Example 3.4 3.7 Example 3.5 3.8 Example 3.6 3.9 Problem 1 3.10 Problem 2 3.11 Problem 3 3.12 Problem 4 3.13 Problem 5 3.14 Problem 7 3.15 Problem 9 3.16 Problem 11 3.17 C1 3.18 C2 3.19 C3 3.20 C5 3.21 C9", " Chapter 3 Multiple regression Analysis: estimation 3.1 Formula \\[wage = \\beta_0 + \\beta_1 * educ + \\beta_2 * exper + u\\] The coefficient of interest for policy purposes is \\(\\beta_1\\), the ceteris paribus effect of expend on avgscore \\[avgscore = \\beta_0 + \\beta_1 * expend + \\beta_2 * avginc + u\\] \\(\\beta_2\\), measures the ceteris paribus effect of exper on wage \\[cons = \\beta_0 + \\beta_1 * inc + \\beta_2 * inc^{2} + u\\] \\[\\tilde{\\beta_1} = \\hat{\\beta_1} + \\hat{\\beta_2} + \\tilde{\\delta_1} \\] The simple regression coefficient \\(\\tilde{\\beta_1}\\) does not usually equal the multiple \\(\\hat{\\beta_1}\\). \\(\\tilde{\\delta_1}\\) is the slope coefficient from the simple regression of \\(x_{i2}\\) on \\(x_{i1}\\). \\(\\tilde{\\delta_1}\\) differs from the partial effect of \\(x_1\\) on \\(\\hat{y}\\). The relationship between \\(\\tilde{\\delta_1}\\) and \\(\\hat{\\beta_1}\\) also shows there are two distinct cases where they are equal: 1. The partial effect of \\(x_2\\) on \\(\\hat{y}\\) is zero in the sample. That is, \\(\\hat{\\beta_2}\\) = 0. 2. \\(x_1\\) and \\(x_2\\) are uncorrelated in the sample. That is, \\(\\tilde{\\delta_1}\\) = 0. 3.2 Explore A simple model to explain city murder rates (murdrate) in terms of the probability of conviction (prbconv) and average sentence length (avgsen) is \\[murdrate = \\beta_0 + \\beta_1 * prbconv + \\beta_2 * avgsen + u\\] What are some factors contained in u? Do you think the key assumption (3.5) is likely to hold? In Example 3.1, the OLS fitted line explaining college GPA in terms of high school GPA and ACT score is \\[colGPA = 1.29 + .453 * hsGPA + .0094 * ACT\\] If the average high school GPA is about 3.4 and the average ACT score is about 24.2, what is the average college GPA in the sample? ## ## Call: ## lm(formula = colGPA ~ hsGPA + ACT, data = gpa1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.85442 -0.24666 -0.02614 0.28127 0.85357 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.286328 0.340822 3.774 0.000238 *** ## hsGPA 0.453456 0.095813 4.733 5.42e-06 *** ## ACT 0.009426 0.010777 0.875 0.383297 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3403 on 138 degrees of freedom ## Multiple R-squared: 0.1764, Adjusted R-squared: 0.1645 ## F-statistic: 14.78 on 2 and 138 DF, p-value: 1.526e-06 ## $fit ## 1 ## 3.056187 ## ## $se.fit ## [1] 0.02866556 ## ## $df ## [1] 138 ## ## $residual.scale ## [1] 0.3403158 In the previous example, if we use as explanatory variables expendA, expendB, and shareA, where shareA = 100(expendA/ totexpend) is the percentage share of total campaign expenditures made by Candidate A, does this violate Assumption MLR.3? 3.3 Example 3.1 The variables in GPA1 include the college grade point average (colGPA), high school GPA (hsGPA), and achievement test score (ACT) for a sample of 141 students from a large university; both college and high school GPAs are on a four-point scale. We obtain the following OLS regression line to predict college GPA from high school GPA and achievement test score: ## ## Call: ## lm(formula = colGPA ~ hsGPA + ACT, data = gpa1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.85442 -0.24666 -0.02614 0.28127 0.85357 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.286328 0.340822 3.774 0.000238 *** ## hsGPA 0.453456 0.095813 4.733 5.42e-06 *** ## ACT 0.009426 0.010777 0.875 0.383297 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3403 on 138 degrees of freedom ## Multiple R-squared: 0.1764, Adjusted R-squared: 0.1645 ## F-statistic: 14.78 on 2 and 138 DF, p-value: 1.526e-06 The intercept 1.29 is the predicted college GPA if hsGPA and ACT are both set as zero. Since no one who attends college has either a zero high school GPA or a zero on the achievement test. hsGPA is associated with .453 of a point on the college GPA, or almost half a point. The sign on ACT implies that, while holding hsGPA fixed, a change in the ACT score of 10 points—a very large change, since the maximum ACT score is 36 and the average score in the sample is about 24 with a standard deviation less than three—affects colGPA by less than one-tenth of a point. The ACT score is not a strong predictor of college GPA and statistically insignificant. ## ## Call: ## lm(formula = colGPA ~ ACT, data = gpa1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.85251 -0.25251 -0.04426 0.26400 0.89336 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.40298 0.26420 9.095 8.8e-16 *** ## ACT 0.02706 0.01086 2.491 0.0139 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3656 on 139 degrees of freedom ## Multiple R-squared: 0.04275, Adjusted R-squared: 0.03586 ## F-statistic: 6.207 on 1 and 139 DF, p-value: 0.0139 3.4 Example 3.2 Using the 526 observations on workers in WAGE1, we include educ (years of education), exper (years of labor market experience), and tenure (years with the current employer) in an equation explaining log(wage). The estimated equation is \\[log(wage) = 0.284 + 0.092 educ + 0.0041 exper + 0.022 tenure\\] ## ## Call: ## lm(formula = lwage ~ exper + tenure, data = wage1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.18698 -0.36511 -0.05057 0.31486 1.45326 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.533650 0.035222 43.542 &lt; 2e-16 *** ## exper -0.002669 0.001865 -1.431 0.153 ## tenure 0.026455 0.003504 7.550 1.95e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5026 on 523 degrees of freedom ## Multiple R-squared: 0.1095, Adjusted R-squared: 0.1061 ## F-statistic: 32.14 on 2 and 523 DF, p-value: 6.825e-14 The coefficients have a percentage interpretation. The coefficient 0.092 means that, holding exper and tenure fixed, another year of education is predicted to increase log(wage) by 0.092, which translates into an approximate 9.2% increase in wage. The coefficient 0.0041 means that, holding educ and tenure fixed, another year of experience is predicted to increase log(wage) by 0.0041, which translates into an approximate 0.4% increase in wage. The coefficient 0.022 means that, holding educ and exper fixed, another year of tenure is predicted to increase log(wage) by 0.022, which translates into an approximate 2.2% increase in wage. 3.5 Example 3.3 We use the data in 401K to estimate the effect of a plan’s match rate (mrate) on the participation rate (prate) in its 401(k) pension plan. The match rate is the amount the firm contributes to a worker’s fund for each dollar the worker contributes (up to some limit); thus, mrate 5 .75 means that the firm contributes 75¢ for each dollar contributed by the worker. The participation rate is the percentage of eligible workers having a 401(k) account. The variable age is the age of the 401(k) plan. There are 1,534 plans in the data set, the average prate is 87.36, the average mrate is .732, and the average age is 13.2. Regressing prate on mrate gives \\[prate = 83.076 + 5.861 mrate\\] Regressing prate on mrate, age gives \\[prate = 80.12 + 5.52 mrate + 243 age\\] ## ## Call: ## lm(formula = prate ~ mrate, data = k401k) ## ## Residuals: ## Min 1Q Median 3Q Max ## -82.303 -8.184 5.178 12.712 16.807 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 83.0755 0.5633 147.48 &lt;2e-16 *** ## mrate 5.8611 0.5270 11.12 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 16.09 on 1532 degrees of freedom ## Multiple R-squared: 0.0747, Adjusted R-squared: 0.0741 ## F-statistic: 123.7 on 1 and 1532 DF, p-value: &lt; 2.2e-16 ## ## Call: ## lm(formula = prate ~ mrate + age, data = k401k) ## ## Residuals: ## Min 1Q Median 3Q Max ## -81.162 -8.067 4.787 12.474 18.256 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 80.1191 0.7790 102.85 &lt; 2e-16 *** ## mrate 5.5213 0.5259 10.50 &lt; 2e-16 *** ## age 0.2432 0.0447 5.44 6.21e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.94 on 1531 degrees of freedom ## Multiple R-squared: 0.09225, Adjusted R-squared: 0.09106 ## F-statistic: 77.79 on 2 and 1531 DF, p-value: &lt; 2.2e-16 The simple regression estimate of the effect of mrate on prate is clearly different from the multiple regression estimate, but the difference is not very big. The simple regression estimate is only about 6.2% larger than the multiple regression estimate. This can be explained by the fact that the sample correlation between mrate and age is only 0.12. ## [1] 0.1187841 3.6 Example 3.4 ## ## Call: ## lm(formula = colGPA ~ hsGPA + ACT, data = gpa1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.85442 -0.24666 -0.02614 0.28127 0.85357 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.286328 0.340822 3.774 0.000238 *** ## hsGPA 0.453456 0.095813 4.733 5.42e-06 *** ## ACT 0.009426 0.010777 0.875 0.383297 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3403 on 138 degrees of freedom ## Multiple R-squared: 0.1764, Adjusted R-squared: 0.1645 ## F-statistic: 14.78 on 2 and 138 DF, p-value: 1.526e-06 This means that hsGPA and ACT together explain about 17.6% of the variation in college GPA for this sample of students. This may not seem like a high percentage, but we must remember that there are many other factors—including family background, personality, quality of high school education, affinity for college—that contribute to a student’s college performance. If hsGPA and ACT explained almost all of the variation in colGPA, then performance in college would be preordained by high school performance! 3.7 Example 3.5 CRIME1 contains data on arrests during the year 1986 and other information on 2,725 men born in either 1960 or 1961 in California. Each man in the sample was arrested at least once prior to 1986. The variable narr86 is the number of times the man was arrested during 1986: it is zero for most men in the sample (72.29%), and it varies from 0 to 12. (The percentage of men arrested once during 1986 was 20.51.) The variable pcnv is the proportion (not percentage) of arrests prior to 1986 that led to conviction, avgsen is average sentence length served for prior convictions (zero for most people), ptime86 is months spent in prison in 1986, and qemp86 is the number of quarters during which the man was employed in 1986 (from zero to four). A linear model explaining arrests is ## ## Call: ## lm(formula = narr86 ~ pcnv + avgsen + ptime86 + qemp86, data = crime1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.9330 -0.4247 -0.2934 0.3506 11.4403 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.706756 0.033151 21.319 &lt; 2e-16 *** ## pcnv -0.150832 0.040858 -3.692 0.000227 *** ## avgsen 0.007443 0.004734 1.572 0.115993 ## ptime86 -0.037391 0.008794 -4.252 2.19e-05 *** ## qemp86 -0.103341 0.010396 -9.940 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8414 on 2720 degrees of freedom ## Multiple R-squared: 0.04219, Adjusted R-squared: 0.04079 ## F-statistic: 29.96 on 4 and 2720 DF, p-value: &lt; 2.2e-16 This equation says that, as a group, the three variables pcnv, ptime86, and qemp86 explain about 4.1% of the variation in narr86. ## ## Call: ## lm(formula = narr86 ~ pcnv + ptime86 + qemp86, data = crime1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.7118 -0.4031 -0.2953 0.3452 11.4358 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.711772 0.033007 21.565 &lt; 2e-16 *** ## pcnv -0.149927 0.040865 -3.669 0.000248 *** ## ptime86 -0.034420 0.008591 -4.007 6.33e-05 *** ## qemp86 -0.104113 0.010388 -10.023 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8416 on 2721 degrees of freedom ## Multiple R-squared: 0.04132, Adjusted R-squared: 0.04027 ## F-statistic: 39.1 on 3 and 2721 DF, p-value: &lt; 2.2e-16 This equation says that, as a group, the three variables pcnv, avgsen, ptime86, and qemp86 explain about 4.1% of the variation in narr86. Adding the average sentence variable increases \\(R^2\\) from .0413 to .0422, a practically small effect. The sign of the coefficient on avgsen is also unexpected: it says that a longer average sentence length increases criminal activity. ## ## Call: ## lm(formula = narr86 ~ pcnv + ptime86 + qemp86, data = crime1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.7118 -0.4031 -0.2953 0.3452 11.4358 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.711772 0.033007 21.565 &lt; 2e-16 *** ## pcnv -0.149927 0.040865 -3.669 0.000248 *** ## ptime86 -0.034420 0.008591 -4.007 6.33e-05 *** ## qemp86 -0.104113 0.010388 -10.023 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8416 on 2721 degrees of freedom ## Multiple R-squared: 0.04132, Adjusted R-squared: 0.04027 ## F-statistic: 39.1 on 3 and 2721 DF, p-value: &lt; 2.2e-16 3.8 Example 3.6 ## ## Call: ## lm(formula = lwage ~ educ, data = wage1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.21158 -0.36393 -0.07263 0.29712 1.52339 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.583773 0.097336 5.998 3.74e-09 *** ## educ 0.082744 0.007567 10.935 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4801 on 524 degrees of freedom ## Multiple R-squared: 0.1858, Adjusted R-squared: 0.1843 ## F-statistic: 119.6 on 1 and 524 DF, p-value: &lt; 2.2e-16 3.9 Problem 1 Using the data in GPA2 on 4,137 college students, the following equation was estimated by OLS: \\[colgpa = 1.392 - .0135 hsperc + .00148 sat \\] \\[n = 4137, R^{2} = .273\\] where colgpa is measured on a four-point scale, hsperc is the percentile in the high school graduating class (defined so that, for example, hsperc = 5 means the top 5% of the class), and sat is the combined math and verbal scores on the student achievement test. Why does it make sense for the coefficient on hsperc to be negative? hsperc is defined so that the smaller it is, the higher the student’s standing in high school. Everything else equal, the worse the student’s standing in high school, the expected college GPA is lower. hsperc is the percentile in the high school graduating class. As the percentile increases we expecte the college GPA to decrease. What is the predicted college GPA when hsperc = 20 and sat = 1,050? ## ## Call: ## lm(formula = colgpa ~ hsperc + sat, data = gpa2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.6007 -0.3581 0.0329 0.3963 1.7599 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.392e+00 7.154e-02 19.45 &lt;2e-16 *** ## hsperc -1.352e-02 5.495e-04 -24.60 &lt;2e-16 *** ## sat 1.476e-03 6.531e-05 22.60 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5615 on 4134 degrees of freedom ## Multiple R-squared: 0.2734, Adjusted R-squared: 0.2731 ## F-statistic: 777.9 on 2 and 4134 DF, p-value: &lt; 2.2e-16 ## $fit ## 1 ## 2.671407 ## ## $se.fit ## [1] 0.008851742 ## ## $df ## [1] 4134 ## ## $residual.scale ## [1] 0.5615457 Suppose that two high school graduates, A and B, graduated in the same percentile from high school, but Student A’s SAT score was 140 points higher (about one standard deviation in the sample). What is the predicted difference in college GPA for these two students? Is the difference large? The difference between A and B is simply 140 times the coefficient on sat, because hsperc is the same for both students. So A is predicted to have colgpa 0.00148 (140) \\(\\sim\\) .207 higher. Holding hsperc fixed, what difference in SAT scores leads to a predicted colgpa difference of .50, or one-half of a grade point? Comment on your answer. \\(\\Delta colgpa\\) = .5, so .5 = 0.00148 \\(\\Delta sat\\), \\(\\Delta sat\\) = 337.8378378 A difference in SAT score of roughly two and one-half standard deviations is required to get predicted difference in college GPA of a half a point. ## [1] 139.4014 ## $fit ## 1 2 ## 3.148463 3.647187 ## ## $se.fit ## 1 2 ## 0.01530641 0.03239112 ## ## $df ## [1] 4134 ## ## $residual.scale ## [1] 0.5615457 3.10 Problem 2 The data in WAGE2 on working men was used to estimate the following equation: \\[educ = 10.36 - .094 sibs + .131 meduc + .210 feduc\\] \\[n = 722, R^{2} = .214\\] where educ is years of schooling, sibs is number of siblings, meduc is mother’s years of schooling, and feduc is father’s years of schooling. Does sibs have the expected effect? Explain. Holding meduc and feduc fixed, by how much does sibs have to increase to reduce predicted years of education by one year? (A noninteger answer is acceptable here.) ## ## Call: ## lm(formula = educ ~ sibs + meduc + feduc, data = wage2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.0906 -1.5957 -0.3677 1.6138 5.6103 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.36426 0.35850 28.910 &lt; 2e-16 *** ## sibs -0.09364 0.03447 -2.716 0.00676 ** ## meduc 0.13079 0.03269 4.001 6.96e-05 *** ## feduc 0.21000 0.02747 7.644 6.79e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.987 on 718 degrees of freedom ## (213 observations deleted due to missingness) ## Multiple R-squared: 0.2141, Adjusted R-squared: 0.2108 ## F-statistic: 65.2 on 3 and 718 DF, p-value: &lt; 2.2e-16 An increase in number of siblings (sibs) is associated with an 0.0936 decrease in the average monthly earnings and this effect is is statistically significant at 1%. Discuss the interpretation of the coefficient on meduc. A year increase in mother’s education (meduc) is associated with a 0.131 increase in the average monthly earnings and this effect is is statistically significant at 1%. Suppose that Man A has no siblings, and his mother and father each have 12 years of education. Man B has no siblings, and his mother and father each have 16 years of education. What is the predicted difference in years of education between B and A? The predicted difference in years of education between B and A is 1.363165. 3.11 Problem 3 The following model is a simplified version of the multiple regression model used by Biddle and Hamermesh (1990) to study the tradeoff between time spent sleeping and working and to look at other factors affecting sleep: \\[sleep = \\beta_0 + \\beta_1 * totwrk + \\beta_2 * educ + \\beta_3 * age + u\\] where sleep and totwrk (total work) are measured in minutes per week and educ and age are measured in years. (See also Computer Exercise C3 in Chapter 2.) If adults trade off sleep for work, what is the sign of \\(\\beta1\\)? The sign would be negative if adults trade off sleep for work. More work implies less sleep (other things equal), so \\(\\beta1\\) &lt; 0. ## ## Call: ## lm(formula = sleep ~ totwrk + educ + age, data = sleep75) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2405.12 -236.50 14.53 253.87 1303.42 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3638.24531 112.27511 32.405 &lt;2e-16 *** ## totwrk -0.14837 0.01669 -8.888 &lt;2e-16 *** ## educ -11.13381 5.88457 -1.892 0.0589 . ## age 2.19988 1.44572 1.522 0.1285 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 419.4 on 702 degrees of freedom ## Multiple R-squared: 0.1134, Adjusted R-squared: 0.1096 ## F-statistic: 29.92 on 3 and 702 DF, p-value: &lt; 2.2e-16 What signs do you think \\(\\beta2\\) and \\(\\beta3\\) will have? The sign could probably be negative for adults’ education. More educated people like to get more out of life (other things equal) so they sleep less (\\(\\beta2\\) &lt; 0). The sign could probably be positive for adults’ age. Older people might want to sleep more (other things equal) (\\(\\beta3\\) &lt; 0). Using the data in SLEEP75, the estimated equation is \\[sleep = 3638.25 - .148 * totwrk - 11.13 * educ + 2.20 * age \\] \\[n = 706, R^{2} = .113\\] If someone works five more hours per week, by how many minutes is sleep predicted to fall? Is this a large tradeoff? ## [1] -44.4 totwrk is in minutes, five hours must be converted into minutes: \\(\\Delta\\) totwrk = 5(60) = 300. sleep is predicted to decrease by -0.148 (300) = 44.4 minutes. For a week, 45 minutes less sleep is not an overwhelming change therefore it is not a large tradeoff. Discuss the sign and magnitude of the estimated coefficient on educ. More education implies less predicted time sleeping, but the effect is small. Assume the difference between college and high school is four years, the college graduate sleeps about 35.52 minutes less per week holding other things equal. ## [1] -35.52 Would you say totwrk, educ, and age explain much of the variation in sleep? What other factors might affect the time spent sleeping? Are these likely to be correlated with totwrk? The Adjusted \\(R^{2}\\) value is 0.1096. This tells us that 10.96% of the variation in mins sleep at night(per week), as quantified by sleep, is explained by totwrk, educ, age. Other factors might affect the time spent sleeping: health, marital status and number of children 3.12 Problem 4 3.13 Problem 5 In a study relating college grade point average to time spent in various activities, you distribute a survey to several students. The students are asked how many hours they spend each week in four activities: studying, sleeping, working, and leisure. Any activity is put into one of the four categories, so that for each student, the sum of hours in the four activities must be 168. In the model \\[GPA = \\beta_0 + \\beta_1 * study + \\beta_2 * sleep + \\beta_3 * work + \\beta_4 * leisure + u\\] does it make sense to hold sleep, work, and leisure fixed, while changing study? study + sleep + work + leisure = 168. If we change study, at least one of the other categories must change so that the sum = 168. Explain why this model violates Assumption MLR.3. study is perfect linear function of the other independent variables study = 168 - sleep - work - leisure. How could you reformulate the model so that its parameters have a useful interpretation and it satisfies Assumption MLR.3? One option is to drop one of the independent variables such as leisure: \\[GPA = \\beta_0 + \\beta_1 * study + \\beta_2 * sleep + \\beta_3 * work + u\\] \\(\\beta_1\\) is interpreted as the change in GPA when study increases by one hour, where sleep, work, and u are all held fixed. If we are holding sleep and work fixed but increasing study by one hour, then we must be reducing leisure by one hour. The other slope parameters have a similar interpretation. 3.14 Problem 7 Which of the following can cause OLS estimators to be biased? Heteroskedasticity Omitting an important variable omitting an important variable can cause bias and this is true only when the omitted variable is correlated with the included explanatory variables. A sample correlation coefficient of .95 between two independent variables both included in the model. 3.15 Problem 9 3.16 Problem 11 The following equation describes the median housing price in a community in terms of amount of pollution (nox for nitrous oxide) and the average number of rooms in houses in the community (rooms): \\[log(price) = \\beta_0 + \\beta_1 * log(nox) + \\beta_2 * rooms + u\\] What are the probable signs of \\(\\beta_1\\) and \\(\\beta_2\\)? What is the interpretation of \\(\\beta_1\\)? Explain. ## ## Call: ## lm(formula = lprice ~ lnox + rooms, data = hprice2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.06485 -0.12331 0.00782 0.14471 1.38770 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.23374 0.18774 49.18 &lt;2e-16 *** ## lnox -0.71767 0.06634 -10.82 &lt;2e-16 *** ## rooms 0.30592 0.01902 16.09 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.286 on 503 degrees of freedom ## Multiple R-squared: 0.5137, Adjusted R-squared: 0.5118 ## F-statistic: 265.7 on 2 and 503 DF, p-value: &lt; 2.2e-16 \\(\\beta_1\\) &lt; 0 because more pollution can be expected to lower housing values. \\(\\beta_1\\) is the elasticity of price with respect to nox. \\(\\beta_2\\) &gt; 0 because rooms is highly related to the size of a house. \\(\\beta_2\\) is the elasticity of price with respect to rooms Why might nox [or more precisely, log(nox)] and rooms be negatively correlated? If this is the case, does the simple regression of log(price) on log(nox) produce an upward or a downward biased estimator of \\(\\beta_1\\)? ## [1] -0.3049879 ## ## Call: ## lm(formula = lprice ~ lnox, data = hprice2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.17062 -0.19368 -0.02582 0.18385 1.09366 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.70719 0.13243 88.40 &lt;2e-16 *** ## lnox -1.04314 0.07767 -13.43 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3516 on 504 degrees of freedom ## Multiple R-squared: 0.2636, Adjusted R-squared: 0.2621 ## F-statistic: 180.4 on 1 and 504 DF, p-value: &lt; 2.2e-16 ## ## Call: ## lm(formula = lprice ~ rooms, data = hprice2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.20213 -0.09645 0.06184 0.16928 1.36319 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.62435 0.12702 60.02 &lt;2e-16 *** ## rooms 0.36866 0.02009 18.35 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3172 on 504 degrees of freedom ## Multiple R-squared: 0.4006, Adjusted R-squared: 0.3994 ## F-statistic: 336.8 on 1 and 504 DF, p-value: &lt; 2.2e-16 Assume that rooms increases with quality of the home/neighborhood , then log(nox) and rooms are negatively correlated when poorer/less desirable neighborhoods have more pollution. Using the data in HPRICE2, the following equations were estimated: \\[log(price) = 11.71 - 1.043 * log(nox)\\] \\[n = 506, R^{2} = 0.264\\] \\[log(price) = 9.23 - 0.718 * log(nox) + 0.306 * rooms + u\\] \\[n = 506, R^{2} = 0.514\\] Is the relationship between the simple and multiple regression estimates of the elasticity of price with respect to nox what you would have predicted, given your answer in part? (ii) Does this mean that -0.718 is definitely closer to the true elasticity than -1.043? The relationship between the simple and multiple regression estimates of the elasticity of price with respect to nox is what was expect from the analysis in part (ii). The simple regression estimate, 1.043, is more negative (larger in magnitude) than the multiple regression estimate, 0.718. \\(\\beta_1\\) is closer to 0.718. 3.17 C1 A problem of interest to health officials (and others) is to determine the effects of smoking during pregnancy on infant health. One measure of infant health is birth weight; a birth weight that is too low can put an infant at risk for contracting various illnesses. Since factors other than cigarette smoking that affect birth weight are likely to be cor- related with smoking, we should take those factors into account. For example, higher income generally results in access to better prenatal care, as well as better nutrition for the mother. An equation that recognizes this is \\[bwght = \\hat{\\beta}_0 + \\hat{\\beta}_1 * b1cigs + \\hat{\\beta}_2 * faminc + u\\] What is the most likely sign for \\(\\beta_2\\)? \\(\\beta_2\\) &gt; 0, as more income typically means better nutrition for the mother and better prenatal care. This in turns means higher birth weight. Do you think cigs and faminc are likely to be correlated? Explain why the correlation might be positive or negative. ## cigs faminc ## cigs 1.0000000 -0.1730449 ## faminc -0.1730449 1.0000000 Now, estimate the equation with and without faminc, using the data in BWGHT. Report the results in equation form, including the sample size and R-squared. Discuss your results, focusing on whether adding faminc substantially changes the estimated effect of cigs on bwght. \\[bwght = 116.97 - 0.463 * cigs + 0.093 * faminc\\] ## ## Call: ## lm(formula = bwght ~ cigs + faminc, data = bwght) ## ## Residuals: ## Min 1Q Median 3Q Max ## -96.061 -11.543 0.638 13.126 150.083 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 116.97413 1.04898 111.512 &lt; 2e-16 *** ## cigs -0.46341 0.09158 -5.060 4.75e-07 *** ## faminc 0.09276 0.02919 3.178 0.00151 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20.06 on 1385 degrees of freedom ## Multiple R-squared: 0.0298, Adjusted R-squared: 0.0284 ## F-statistic: 21.27 on 2 and 1385 DF, p-value: 7.942e-10 Sample size is 1388. \\(R^{2}\\) value is 0.0298. Adjusted \\(R^{2}\\) value is 0.0284. \\[bwght = 119.77 - 0.514 * cigs\\] ## ## Call: ## lm(formula = bwght ~ cigs, data = bwght) ## ## Residuals: ## Min 1Q Median 3Q Max ## -96.772 -11.772 0.297 13.228 151.228 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 119.77190 0.57234 209.267 &lt; 2e-16 *** ## cigs -0.51377 0.09049 -5.678 1.66e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20.13 on 1386 degrees of freedom ## Multiple R-squared: 0.02273, Adjusted R-squared: 0.02202 ## F-statistic: 32.24 on 1 and 1386 DF, p-value: 1.662e-08 Sample size is 1388. \\(R^{2}\\) value is 0.0227. Adjusted \\(R^{2}\\) value is 0.022. The effect of cigarette smoking is slightly larger when faminc is removed from the regression, but the difference is small. This is probably due to the fact that cigs and faminc are not very correlated, and the coefficient on faminc is practically small. (The variable faminc is measured in thousands, so $10,000 more in 1988 income increases predicted birth weight by only .93 ounces.) ## $fit ## 1 2 3 ## 119.7719 119.2581 114.6342 ## ## $se.fit ## 1 2 3 ## 0.5723407 0.5491633 0.8970024 ## ## $df ## [1] 1386 ## ## $residual.scale ## [1] 20.12858 3.18 C2 Use the data in HPRICE1 to estimate the model \\[price = \\beta_0 + \\beta_1 * sqrft + \\beta_2 * bdrms + u\\] where price is the house price measured in thousands of dollars. Write out the results in equation form. ## ## Call: ## lm(formula = price ~ sqrft + bdrms, data = hprice1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -127.627 -42.876 -7.051 32.589 229.003 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -19.31500 31.04662 -0.622 0.536 ## sqrft 0.12844 0.01382 9.291 1.39e-14 *** ## bdrms 15.19819 9.48352 1.603 0.113 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 63.04 on 85 degrees of freedom ## Multiple R-squared: 0.6319, Adjusted R-squared: 0.6233 ## F-statistic: 72.96 on 2 and 85 DF, p-value: &lt; 2.2e-16 \\[price = \\beta_0 + \\beta_1 * sqrft + \\beta_2 * bdrms + u\\] What is the estimated increase in price for a house with one more bedroom, holding square footage constant? The estimated increase in price for a house with one more bedroom while holding square footage constant is 15.2 What is the estimated increase in price for a house with an additional bedroom that is 140 square feet in size? Compare this to your answer in part (ii). ## [1] 17.92 What percentage of the variation in price is explained by square footage and number of bedrooms? The Adjusted R2 value is 0.6233. This tells us that 62.33% of the variation in house price ($1000s), as quantified by price, is explained by sqrft, bdrms. ## $fit ## 1 2 ## -4.116805 -1.333926 ## ## $se.fit ## 1 2 ## 26.31783 30.48764 ## ## $df ## [1] 85 ## ## $residual.scale ## [1] 63.04484 The first house in the sample has sqrft = 2,438 and bdrms = 4. Find the predicted selling price for this house from the OLS regression line. ## $fit ## 1 ## 354.6052 ## ## $se.fit ## [1] 8.41493 ## ## $df ## [1] 85 ## ## $residual.scale ## [1] 63.04484 ## [1] 300 The actual selling price of the first house in the sample was $300,000 (so price = 300). Find the residual for this house. Does it suggest that the buyer underpaid or overpaid for the house? ## 1 ## -54.60525 3.19 C3 The file CEOSAL2 contains data on 177 chief executive officers and can be used to examine the effects of firm performance on CEO salary. Estimate a model relating annual salary to firm sales and market value. Make the model of the constant elasticity variety for both independent variables. Write the results out in equation form. ## ## Call: ## lm(formula = lsalary ~ lsales + lmktval, data = ceosal2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.28060 -0.31137 -0.01269 0.30645 1.91210 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.62092 0.25441 18.163 &lt; 2e-16 *** ## lsales 0.16213 0.03967 4.087 6.67e-05 *** ## lmktval 0.10671 0.05012 2.129 0.0347 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5103 on 174 degrees of freedom ## Multiple R-squared: 0.2991, Adjusted R-squared: 0.2911 ## F-statistic: 37.13 on 2 and 174 DF, p-value: 3.727e-14 \\[price = 4.62 + 0.162 * lsales + 0.107 * lmktval\\] \\[n = 177, R^{2} = 0.299\\] Add profits to the model from part (i). Why can this variable not be included in logarithmic form? Would you say that these firm performance variables explain most of the variation in CEO salaries? profits cannotbe included in logarithmic form because profits are negative for nine of the companies in the sample. ## ## Call: ## lm(formula = lsalary ~ lsales + lmktval + profits, data = ceosal2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.27002 -0.31026 -0.01027 0.31043 1.91489 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.687e+00 3.797e-01 12.343 &lt; 2e-16 *** ## lsales 1.614e-01 3.991e-02 4.043 7.92e-05 *** ## lmktval 9.753e-02 6.369e-02 1.531 0.128 ## profits 3.566e-05 1.520e-04 0.235 0.815 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5117 on 173 degrees of freedom ## Multiple R-squared: 0.2993, Adjusted R-squared: 0.2872 ## F-statistic: 24.64 on 3 and 173 DF, p-value: 2.53e-13 \\[price = 4.69 + 0.161 * lsales + 0.098 * lmktval + 0.000036 * profits\\] \\[n = 177, R^{2} = 0.299\\] The coefficient on profits (measured in millions) is very small. If profits increase by $1 billion, \\(\\Delta profits\\) = 1,000 the predicted salary increases by about only 3.6% while holding sales and market value fixed. profits can be dropped without losing variation TBD. The model explain almost 30% of the sample variation in log(salary). ## [1] 0.0357 Add the variable ceoten to the model in part (ii). What is the estimated percentage return for another year of CEO tenure, holding other factors fixed? ## ## Call: ## lm(formula = lsalary ~ lsales + lmktval + ceoten, data = ceosal2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.49693 -0.29472 0.00964 0.30417 1.85286 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.503795 0.257234 17.509 &lt; 2e-16 *** ## lsales 0.162854 0.039242 4.150 5.21e-05 *** ## lmktval 0.109243 0.049595 2.203 0.0289 * ## ceoten 0.011705 0.005326 2.198 0.0293 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5048 on 173 degrees of freedom ## Multiple R-squared: 0.3182, Adjusted R-squared: 0.3063 ## F-statistic: 26.91 on 3 and 173 DF, p-value: 2.474e-14 one more year as CEO increases predicted salary by about 1.2%. Find the sample correlation coefficient between the variables log(mktval) and profits. Are these variables highly correlated? What does this say about the OLS estimators? ## [1] 0.7768976 The sample correlation between log(mktval) and profits is about 0.7768976, which is high. This causes no bias in the OLS estimators, although it can cause their variances to be large. Given the fairly substantial correlation between market value and firm profits, it is not too surprising that the latter adds nothing to explaining CEO salaries. Also, profits is a short term measure of how the firm is doing while mktval is based on past, current, and expected future profitability. 3.20 C5 Confirm the partialling out interpretation of the OLS estimates by explicitly doing the partialling out for Example 3.2. This first requires regressing educ on exper and tenure and saving the residuals, rˆ1. Then, regress log(wage) on rˆ1. Compare the coefficient on ˆr1 with the coefficient on educ in the regression of log(wage) on educ, exper, and tenure. \\[log(wage) = 0.284 + 0.092 educ + 0.0041 exper + 0.022 tenure\\] ## ## Call: ## lm(formula = lwage ~ educ + exper + tenure, data = wage1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.05802 -0.29645 -0.03265 0.28788 1.42809 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.284360 0.104190 2.729 0.00656 ** ## educ 0.092029 0.007330 12.555 &lt; 2e-16 *** ## exper 0.004121 0.001723 2.391 0.01714 * ## tenure 0.022067 0.003094 7.133 3.29e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4409 on 522 degrees of freedom ## Multiple R-squared: 0.316, Adjusted R-squared: 0.3121 ## F-statistic: 80.39 on 3 and 522 DF, p-value: &lt; 2.2e-16 ## ## Call: ## lm(formula = educ ~ exper + tenure, data = wage1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.4285 -1.3536 -0.2055 1.6550 5.9791 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.574964 0.184324 73.647 &lt; 2e-16 *** ## exper -0.073785 0.009761 -7.559 1.83e-13 *** ## tenure 0.047680 0.018337 2.600 0.00958 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.63 on 523 degrees of freedom ## Multiple R-squared: 0.1013, Adjusted R-squared: 0.09791 ## F-statistic: 29.49 on 2 and 523 DF, p-value: 7.327e-13 The regression of educ on exper and tenure: \\[educ = 13.57 - 0.074 exper + 0.048 tenure\\] \\[n = 526, R^{2} = 0.101\\] ## ## Call: ## lm(formula = lwage ~ r, data = wage1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.12919 -0.32803 -0.07126 0.31626 1.51357 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.62327 0.02066 78.56 &lt;2e-16 *** ## r 0.09203 0.00788 11.68 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4739 on 524 degrees of freedom ## Multiple R-squared: 0.2066, Adjusted R-squared: 0.205 ## F-statistic: 136.4 on 1 and 524 DF, p-value: &lt; 2.2e-16 Now, when we regress log(wage) on \\(r_1\\): \\[lwage = 1.62 + 0.092 r_1\\] \\[n = 526, R^{2} = 0.207\\] ## C7 Use the data in MEAP93 to answer this question. Estimate the model \\[math10 = \\beta_0 + \\beta_1 * lexpend + \\beta_2 * lnchprg + u\\] and report the results in the usual form, including the sample size and R-squared. Are the signs of the slope coefficients what you expected? Explain. ## ## Call: ## lm(formula = math10 ~ lexpend + lnchprg, data = meap93) ## ## Residuals: ## Min 1Q Median 3Q Max ## -24.294 -6.172 -1.293 4.855 43.203 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -20.36075 25.07288 -0.812 0.4172 ## lexpend 6.22969 2.97263 2.096 0.0367 * ## lnchprg -0.30459 0.03536 -8.614 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.526 on 405 degrees of freedom ## Multiple R-squared: 0.1799, Adjusted R-squared: 0.1759 ## F-statistic: 44.43 on 2 and 405 DF, p-value: &lt; 2.2e-16 Sample size is 408. \\(R^{2}\\) value is 0.1799. Adjusted \\(R^{2}\\) value is 0.1759. What do you make of the intercept you estimated in part (i)? In particular, does it make sense to set the two explanatory variables to zero? [Hint: Recall that log(1)=0.] Now run the simple regression of math10 on log(expend), and compare the slope coefficient with the estimate obtained in part (i). Is the estimated spending effect now larger or smaller than in part (i)? ## ## Call: ## lm(formula = math10 ~ lexpend, data = meap93) ## ## Residuals: ## Min 1Q Median 3Q Max ## -22.343 -7.100 -0.914 6.148 39.093 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -69.341 26.530 -2.614 0.009290 ** ## lexpend 11.164 3.169 3.523 0.000475 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.35 on 406 degrees of freedom ## Multiple R-squared: 0.02966, Adjusted R-squared: 0.02727 ## F-statistic: 12.41 on 1 and 406 DF, p-value: 0.0004752 The estimated spending effect, almost double, is larger than it was in part (i). Find the correlation between lexpend = log(expend) and lnchprg. Does its sign make sense to you? ## [1] -0.1927042 The correlation between lexpend and lnchprg is -0.1927042, which means that on average high schools with poorer students spent less per student. Use part (iv) to explain your findings in part (iii). Equation (3.23) \\[\\tilde{\\beta_1} = \\hat{\\beta_1} + \\hat{\\beta_2} + \\tilde{\\delta_1} \\] Because Corr(x1,x2) &lt; 0, which means\\(\\hat{\\beta_2}\\) &lt; 0, and \\(\\tilde{\\delta_1}\\), the simple regression estimate, \\(\\tilde{\\beta_1}\\), is larger than the multiple regression estimate, \\(\\hat{\\beta_1}\\) . Failing to include the poverty rate (lnchprg) leads to an over-estimate of the effect of spending (lexpend). 3.21 C9 Use the data in CHARITY to answer the following questions: Estimate the equation \\[gift = \\beta_0 + \\beta_1 * mailsyear + \\beta_2 * giftlast + \\beta_3 * propresp + u\\] by OLS and report the results in the usual way, including the sample size and R-squared. How does the R-squared compare with that from the simple regression that omits giftlast and propresp? ## ## Call: ## lm(formula = gift ~ mailsyear + giftlast + propresp, data = charity) ## ## Residuals: ## Min 1Q Median 3Q Max ## -52.893 -7.050 -3.650 1.397 241.206 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.551518 0.803034 -5.668 1.54e-08 *** ## mailsyear 2.166259 0.331927 6.526 7.53e-11 *** ## giftlast 0.005927 0.001432 4.138 3.58e-05 *** ## propresp 15.358605 0.874539 17.562 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.43 on 4264 degrees of freedom ## Multiple R-squared: 0.08336, Adjusted R-squared: 0.08271 ## F-statistic: 129.3 on 3 and 4264 DF, p-value: &lt; 2.2e-16 The estimated equation is \\[gift = -4.55 + 2.166 * mailsyear + 0.006 * giftlast + 15.359 * propresp + u\\] Sample size is 4268. \\(R^{2}\\) value is 0.0834. Adjusted \\(R^{2}\\) value is 0.0827. Interpret the coefficient on mailsyear. Is it bigger or smaller than the corresponding simple regression coefficient? ## ## Call: ## lm(formula = gift ~ mailsyear, data = charity) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.287 -7.976 -5.976 2.687 245.999 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.0141 0.7395 2.724 0.00648 ** ## mailsyear 2.6495 0.3431 7.723 1.4e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.96 on 4266 degrees of freedom ## Multiple R-squared: 0.01379, Adjusted R-squared: 0.01356 ## F-statistic: 59.65 on 1 and 4266 DF, p-value: 1.404e-14 Holding giftlast and propresp fixed, one more mailing per year is estimated to increase gifts by 2.17 guilders. The simple regression estimate is 2.65. The multiple regression estimate is smaller. Interpret the coefficient on propresp. Be careful to notice the units of measurement of propresp. Now add the variable avggift to the equation. What happens to the estimated effect of mailsyear? The estimated equation is \\[gift = -7.328 + 1.201 * mailsyear - 0.2601 * giftlast + 16.205 * propresp + 0.527 * avggift +u\\] ## ## Call: ## lm(formula = gift ~ mailsyear + giftlast + propresp + avggift, ## data = charity) ## ## Residuals: ## Min 1Q Median 3Q Max ## -197.016 -5.883 -2.065 3.031 221.256 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.32776 0.75822 -9.664 &lt; 2e-16 *** ## mailsyear 1.20117 0.31242 3.845 0.000122 *** ## giftlast -0.26086 0.01076 -24.251 &lt; 2e-16 *** ## propresp 16.20464 0.81753 19.821 &lt; 2e-16 *** ## avggift 0.52695 0.02108 24.996 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.47 on 4263 degrees of freedom ## Multiple R-squared: 0.2005, Adjusted R-squared: 0.1998 ## F-statistic: 267.3 on 4 and 4263 DF, p-value: &lt; 2.2e-16 ## ## Call: ## lm(formula = gift ~ giftlast, data = charity) ## ## Residuals: ## Min 1Q Median 3Q Max ## -53.895 -7.389 -7.389 2.611 242.384 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.332770 0.232082 31.596 &lt; 2e-16 *** ## giftlast 0.005656 0.001493 3.788 0.000154 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.04 on 4266 degrees of freedom ## Multiple R-squared: 0.003353, Adjusted R-squared: 0.003119 ## F-statistic: 14.35 on 1 and 4266 DF, p-value: 0.0001537 Holding giftlast, propresp and avggift fixed, the effect of mailings is smaller. The estimated effect of mailsyear 1.2 guilders, less than half the effect estimated by simple regression. In the equation from part (iv), what has happened to the coefficient on giftlast? What do you think is happening? Holding mailsyear, propresp and avggift fixed, the effect of mailings is negative. The estimated effect of giftlast -0.261 guilders, less than half the effect estimated by simple regression. "],
["multiple-regression-analysis-inference.html", "Chapter 4 Multiple regression Analysis: inference 4.1 Explore 4.1 4.2 Explore 4.2 4.3 Explore 4.3 4.4 Explore 4.4 4.5 Explore 4.5 4.6 Explore 4.6 4.7 Example 4.1 4.8 Example 4.2 4.9 Example 4.3 4.10 Example 4.4 4.11 Example 4.5 4.12 Example 4.6 4.13 Example 4.7 4.14 Example 4.8 4.15 Example 4.9 4.16 Example 4.10 4.17 Problem 1 4.18 Problem 2 4.19 Problem 3 4.20 Problem 5 4.21 Problem 7 4.22 Problem 9 4.23 Problem 11 4.24 C1 4.25 C2", " Chapter 4 Multiple regression Analysis: inference 4.1 Explore 4.1 Suppose that u is independent of the expla- natory variables, and it takes on the values 22, 21, 0, 1, and 2 with equal probability of 1/5. Does this violate the Gauss-Markov assumptions? Does this violate the CLM assumptions? 4.2 Explore 4.2 Let community loan approval rates be determined by \\[apprate = \\beta_0 + \\beta_1 * percmin + \\beta_2 * avginc + \\beta_3 * avgwlth + \\beta_4 * avgdebt + u\\], where percmin is the percentage minority in the community, avginc is average income, avgwlth is average wealth, and avgdebt is some measure of average debt obligations. How do you state the null hypothesis that there is no difference in loan rates across neighborhoods due to racial and ethnic composition, when average income, average wealth, and average debt have been controlled for? How do you state the alternative that there is discrimination against minorities in loan approval rates? \\({H_0}: b_{percmin} = 0\\) \\({H_1}: b_{percmin} &lt; 0\\) 4.3 Explore 4.3 Suppose you estimate a regression model and obtain \\(\\hat\\beta_1\\) = .56 and p-value = .086 for testing H0: \\(\\beta_1\\) = 0 against H1: \\(\\beta_1 \\ne\\) 0. What is the p-value for testing H0: \\(\\hat\\beta_1\\) = 0 against H1: \\(\\beta_1\\) &gt; 0? It is simple to obtain the one-sided p-value: just divide the two-sided p-value by 2. ## [1] 0.043 4.4 Explore 4.4 Consider relating individual performance on a standardized test, score, to a variety of other variables. School factors include average class size, per-student expenditures, average teacher compensation, and total school enrollment. Other variables specific to the student are family income, mother’s education, father’s education, and number of siblings. The model is \\[score = \\beta_0 + \\beta_1 * classize + \\beta_2 * expend + \\beta_3 * tchcomp + \\beta_4 * enroll + \\beta_5 * faminc + \\beta_6 * motheduc + \\beta_7 * fatheduc + \\beta_8 * siblings + u\\] State the null hypothesis that student-specific variables have no effect on standardized test performance once school-related factors have been controlled for. What are k and q for this example? Write down the restricted version of the model. \\({H_0}: b_{faminc} = 0, b_{motheduc} = 0, b_{fatheduc} = 0, b_{siblings} = 0\\) \\({H_1}: {H_0}\\) is not true (at least one of \\(\\beta_5,\\beta_6,\\beta_7,\\beta_8\\), is different from zero.) k = 8 q = 4 \\[score = \\beta_0 + \\beta_1 * classize + \\beta_2 * expend + \\beta_3 * tchcomp + \\beta_4 * enroll + u\\] 4.5 Explore 4.5 The data in ATTEND.RAW were used to estimate the two equations atndrte5 47.13 1 13.37 priGPA (2.87) (1.09) n5680,R2 5.183 and atndrte 5 75.70 1 17.26 priGPA 2 1.72 ACT (3.88) (1.08) (?) n 5 680, R2 5 .291, where, as always, standard errors are in parentheses; the standard error for ACT is missing in the second equation. What is the t statistic for the coefficient on ACT ? (Hint: First compute the F statistic for significance of ACT.) ## ## Call: ## lm(formula = atndrte ~ priGPA, data = attend) ## ## Residuals: ## Min 1Q Median 3Q Max ## -70.957 -6.815 2.948 10.631 30.680 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 47.127 2.873 16.41 &lt;2e-16 *** ## priGPA 13.369 1.087 12.30 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.42 on 678 degrees of freedom ## Multiple R-squared: 0.1825, Adjusted R-squared: 0.1813 ## F-statistic: 151.3 on 1 and 678 DF, p-value: &lt; 2.2e-16 ## ## Call: ## lm(formula = atndrte ~ priGPA + ACT, data = attend) ## ## Residuals: ## Min 1Q Median 3Q Max ## -65.373 -6.765 2.125 9.635 29.615 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 75.700 3.884 19.49 &lt;2e-16 *** ## priGPA 17.261 1.083 15.94 &lt;2e-16 *** ## ACT -1.717 0.169 -10.16 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.38 on 677 degrees of freedom ## Multiple R-squared: 0.2906, Adjusted R-squared: 0.2885 ## F-statistic: 138.7 on 2 and 677 DF, p-value: &lt; 2.2e-16 ## Analysis of Variance Table ## ## Model 1: atndrte ~ priGPA ## Model 2: atndrte ~ priGPA + ACT ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 678 161309 ## 2 677 139981 1 21328 103.15 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## [1] 3.855211 reject reject the hypothesis that droprate and gradrate have no effect on salary. 4.6 Explore 4.6 How does adding droprate and gradrate affect the estimate of the salary-benefits tradeoff? Are these variables jointly significant at the 5% level? What about the 10% level? ## ## Call: ## lm(formula = lsalary ~ bensal + lenroll + lstaff + droprate + ## gradrate, data = meap93) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.28711 -0.08902 -0.01949 0.07255 0.39754 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.7384651 0.2582651 41.579 &lt; 2e-16 *** ## bensal -0.5893195 0.1648739 -3.574 0.000394 *** ## lenroll 0.0881204 0.0073240 12.032 &lt; 2e-16 *** ## lstaff -0.2182779 0.0499503 -4.370 1.58e-05 *** ## droprate -0.0002826 0.0016145 -0.175 0.861113 ## gradrate 0.0009674 0.0006625 1.460 0.145032 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1241 on 402 degrees of freedom ## Multiple R-squared: 0.361, Adjusted R-squared: 0.3531 ## F-statistic: 45.43 on 5 and 402 DF, p-value: &lt; 2.2e-16 ## ## Call: ## lm(formula = lsalary ~ bensal + lenroll + lstaff, data = meap93) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.29251 -0.08885 -0.01611 0.07210 0.39151 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.843840 0.251643 43.092 &lt; 2e-16 *** ## bensal -0.604772 0.165368 -3.657 0.000289 *** ## lenroll 0.087397 0.007346 11.897 &lt; 2e-16 *** ## lstaff -0.222033 0.050077 -4.434 1.19e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1246 on 404 degrees of freedom ## Multiple R-squared: 0.3527, Adjusted R-squared: 0.3479 ## F-statistic: 73.39 on 3 and 404 DF, p-value: &lt; 2.2e-16 ## Analysis of Variance Table ## ## Model 1: lsalary ~ bensal + lenroll + lstaff + droprate + gradrate ## Model 2: lsalary ~ bensal + lenroll + lstaff ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 402 6.1929 ## 2 404 6.2734 -2 -0.080464 2.6116 0.07466 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## [1] 3.018168 fail to rejct reject the hypothesis that droprate and gradrate have no effect on salary. 4.7 Example 4.1 Using the data in WAGE1 gives the estimated equation \\[log(wage) = 0.284 + 0.092 educ + 0.0041 exper + 0.022 tenure\\] ## ## Call: ## lm(formula = lwage ~ educ + exper + tenure, data = wage1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.05802 -0.29645 -0.03265 0.28788 1.42809 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.284360 0.104190 2.729 0.00656 ** ## educ 0.092029 0.007330 12.555 &lt; 2e-16 *** ## exper 0.004121 0.001723 2.391 0.01714 * ## tenure 0.022067 0.003094 7.133 3.29e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4409 on 522 degrees of freedom ## Multiple R-squared: 0.316, Adjusted R-squared: 0.3121 ## F-statistic: 80.39 on 3 and 522 DF, p-value: &lt; 2.2e-16 ## [1] 1.281552 ## [1] 1.644854 ## [1] 2.326348 test whether the return to exper, controlling for educ and tenure, is zero in the population, against the alternative that it is positive. \\({H_0}: b_{exper} = 0\\) \\({H_1}: b_{exper} &gt; 0\\) The t statistic for \\(\\hat{b}_{exper}\\) is \\(t_{exper} = 0.0041/0.0017 \\sim 2.41\\). \\(\\hat{b}_{exper}\\) is statistically significant even at the 1% level. Adding three more years increases log(wage) by 3(.0041) = .0123, so wage is only about 1.2% higher ## [1] 0.01236 Partial effect of experience is positive in the population. 4.8 Example 4.2 ## ## Call: ## lm(formula = math10 ~ totcomp + staff + enroll, data = meap93) ## ## Residuals: ## Min 1Q Median 3Q Max ## -22.235 -7.008 -0.807 6.097 40.689 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.2740209 6.1137938 0.372 0.710 ## totcomp 0.0004586 0.0001004 4.570 6.49e-06 *** ## staff 0.0479199 0.0398140 1.204 0.229 ## enroll -0.0001976 0.0002152 -0.918 0.359 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.24 on 404 degrees of freedom ## Multiple R-squared: 0.05406, Adjusted R-squared: 0.04704 ## F-statistic: 7.697 on 3 and 404 DF, p-value: 5.179e-05 enroll is in accordance with the conjecture that larger schools hamper performance: higher enrollment leads to a lower percentage of students with a passing tenth-grade math score. \\({H_0}: b_{enroll} = 0\\) \\({H_1}: b_{enroll} &lt; 0\\) fail to reject H0 in favor of H1 at the 5% level \\({H_0}: b_{totcomp} = 0\\) \\({H_1}: b_{totcomp} &gt; 0\\) reject H0 in favor of H1 at the 1% level \\({H_0}: b_{staff} = 0\\) \\({H_1}: b_{staff} &gt; 0\\) fail to reject H0 in favor of H1 at the 5% level ## ## Call: ## lm(formula = math10 ~ ltotcomp + lstaff + lenroll, data = meap93) ## ## Residuals: ## Min 1Q Median 3Q Max ## -22.735 -6.838 -0.835 6.139 39.718 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -207.6648 48.7031 -4.264 2.50e-05 *** ## ltotcomp 21.1550 4.0555 5.216 2.92e-07 *** ## lstaff 3.9800 4.1897 0.950 0.3427 ## lenroll -1.2680 0.6932 -1.829 0.0681 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.18 on 404 degrees of freedom ## Multiple R-squared: 0.06538, Adjusted R-squared: 0.05844 ## F-statistic: 9.42 on 3 and 404 DF, p-value: 4.974e-06 changing functional form can affect conclusions 4.9 Example 4.3 ## ## Call: ## lm(formula = colGPA ~ hsGPA + ACT + skipped, data = gpa1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.85698 -0.23200 -0.03935 0.24816 0.81657 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.38955 0.33155 4.191 4.95e-05 *** ## hsGPA 0.41182 0.09367 4.396 2.19e-05 *** ## ACT 0.01472 0.01056 1.393 0.16578 ## skipped -0.08311 0.02600 -3.197 0.00173 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3295 on 137 degrees of freedom ## Multiple R-squared: 0.2336, Adjusted R-squared: 0.2168 ## F-statistic: 13.92 on 3 and 137 DF, p-value: 5.653e-08 The t statistic on hsGPA is 4.38, which is significant at very small significance levels. Thus, we say that “hsGPA is statistically significant at any conventional significance level.” The t statistic on ACT is 1.36, which is not statistically significant at the 10% level against a two-sided alternative. Thus, the variable ACT is practically, as well as statistically, insignificant. The t statistic of skipped is -3.19, so skipped is statistically significant at the 1% significance level. 4.10 Example 4.4 ## ## Call: ## lm(formula = lcrime ~ lenroll, data = campus) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5136 -0.3858 0.1174 0.4363 2.5782 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.6314 1.0335 -6.416 5.44e-09 *** ## lenroll 1.2698 0.1098 11.567 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8946 on 95 degrees of freedom ## Multiple R-squared: 0.5848, Adjusted R-squared: 0.5804 ## F-statistic: 133.8 on 1 and 95 DF, p-value: &lt; 2.2e-16 4.11 Example 4.5 ## ## Call: ## lm(formula = lprice ~ lnox + rooms + stratio, data = hprice2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.02357 -0.13576 0.02406 0.13258 1.40083 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.359458 0.219069 47.288 &lt;2e-16 *** ## lnox -0.645329 0.062580 -10.312 &lt;2e-16 *** ## rooms 0.256727 0.018677 13.746 &lt;2e-16 *** ## stratio -0.050873 0.005926 -8.585 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2673 on 502 degrees of freedom ## Multiple R-squared: 0.576, Adjusted R-squared: 0.5734 ## F-statistic: 227.3 on 3 and 502 DF, p-value: &lt; 2.2e-16 4.12 Example 4.6 ## ## Call: ## lm(formula = prate ~ mrate + age + totemp, data = k401k) ## ## Residuals: ## Min 1Q Median 3Q Max ## -77.698 -8.074 4.716 12.505 30.307 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.029e+01 7.777e-01 103.242 &lt; 2e-16 *** ## mrate 5.442e+00 5.244e-01 10.378 &lt; 2e-16 *** ## age 2.692e-01 4.514e-02 5.963 3.07e-09 *** ## totemp -1.291e-04 3.666e-05 -3.521 0.000443 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.88 on 1530 degrees of freedom ## Multiple R-squared: 0.09954, Adjusted R-squared: 0.09778 ## F-statistic: 56.38 on 3 and 1530 DF, p-value: &lt; 2.2e-16 The smallest t statistic in absolute value is that on the variable totemp: t = -3.25, and this is statistically significant at 1% significance levels. if a firm grows by 10,000 employees, the participation rate falls by 1.3 percentage points. This is a huge increase in number of employees with only a modest effect on the participation rate. The effect is not practically very large. 4.13 Example 4.7 ## ## Call: ## lm(formula = lscrap ~ hrsemp + lsales + lemploy, data = jtrain %&gt;% ## filter(year == 1987 &amp; union == 0)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.6301 -0.7523 -0.4016 0.8697 2.8273 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.45837 5.68677 2.191 0.0380 * ## hrsemp -0.02927 0.02280 -1.283 0.2111 ## lsales -0.96203 0.45252 -2.126 0.0436 * ## lemploy 0.76147 0.40743 1.869 0.0734 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.376 on 25 degrees of freedom ## (97 observations deleted due to missingness) ## Multiple R-squared: 0.2624, Adjusted R-squared: 0.1739 ## F-statistic: 2.965 on 3 and 25 DF, p-value: 0.05134 The t statistic on hrsemp is -1.26, and this as not being large enough in magnitude to conclude that hrsemp is statistically significant at the 5% level. hrsemp is not statistically significant, even using a one-sided alternative. 4.14 Example 4.8 ## ## Call: ## lm(formula = lrd ~ lsales + profmarg, data = rdchem) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.97681 -0.31502 -0.05828 0.39020 1.21783 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.37827 0.46802 -9.355 2.93e-10 *** ## lsales 1.08422 0.06020 18.012 &lt; 2e-16 *** ## profmarg 0.02166 0.01278 1.694 0.101 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5136 on 29 degrees of freedom ## Multiple R-squared: 0.918, Adjusted R-squared: 0.9123 ## F-statistic: 162.2 on 2 and 29 DF, p-value: &lt; 2.2e-16 a 1% increase in sales is associated with a 1.084% increase in R&amp;D spending. The 95% confidence interval for log(sales) is between 0.961 and 1.21. The estimated R&amp;D-sales elasticity is not statistically different from 1 at the 5% level. The 95% confidence interval for the population parameter, bprofmarg, is between -.0045 and .0479 the economic size of the profit margin coefficient is not trivial: holding sales fixed, a one percentage point increase in profmarg is estimated to increase R&amp;D spending by 100(0.02166) = 2.2% ## ## Call: ## lm(formula = lwage ~ jc + univ + exper, data = twoyear) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.10362 -0.28132 0.00551 0.28518 1.78167 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.4723256 0.0210602 69.910 &lt;2e-16 *** ## jc 0.0666967 0.0068288 9.767 &lt;2e-16 *** ## univ 0.0768762 0.0023087 33.298 &lt;2e-16 *** ## exper 0.0049442 0.0001575 31.397 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4301 on 6759 degrees of freedom ## Multiple R-squared: 0.2224, Adjusted R-squared: 0.2221 ## F-statistic: 644.5 on 3 and 6759 DF, p-value: &lt; 2.2e-16 jc and univ have both economically and statistically significant effects on wage. ## ## Call: ## lm(formula = lsalary ~ years + gamesyr + bavg + hrunsyr + rbisyr, ## data = mlb1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.02508 -0.45034 -0.04013 0.47014 2.68924 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.119e+01 2.888e-01 38.752 &lt; 2e-16 *** ## years 6.886e-02 1.211e-02 5.684 2.79e-08 *** ## gamesyr 1.255e-02 2.647e-03 4.742 3.09e-06 *** ## bavg 9.786e-04 1.104e-03 0.887 0.376 ## hrunsyr 1.443e-02 1.606e-02 0.899 0.369 ## rbisyr 1.077e-02 7.175e-03 1.500 0.134 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7266 on 347 degrees of freedom ## Multiple R-squared: 0.6278, Adjusted R-squared: 0.6224 ## F-statistic: 117.1 on 5 and 347 DF, p-value: &lt; 2.2e-16 years and gamesyr are statistically significant, none of the variables bavg, hrunsyr, and rbisyr has a statistically significant t statistic against a two-sided alternative, at the 5% significance level. ## ## Call: ## lm(formula = lsalary ~ years + gamesyr, data = mlb1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.66858 -0.46412 -0.01176 0.49219 2.68829 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.223804 0.108312 103.625 &lt; 2e-16 *** ## years 0.071318 0.012505 5.703 2.5e-08 *** ## gamesyr 0.020174 0.001343 15.023 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7527 on 350 degrees of freedom ## Multiple R-squared: 0.5971, Adjusted R-squared: 0.5948 ## F-statistic: 259.3 on 2 and 350 DF, p-value: &lt; 2.2e-16 ## Analysis of Variance Table ## ## Model 1: lsalary ~ years + gamesyr ## Model 2: lsalary ~ years + gamesyr + bavg + hrunsyr + rbisyr ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 350 198.31 ## 2 347 183.19 3 15.125 9.5503 4.474e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## [1] 2.630641 reject the hypothesis that bavg, hrunsyr, and rbisyr have no effect on salary. ## ## Call: ## lm(formula = lsalary ~ years + gamesyr + bavg + hrunsyr, data = mlb1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.0642 -0.4614 -0.0271 0.4654 2.7216 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.020912 0.265719 41.476 &lt; 2e-16 *** ## years 0.067732 0.012113 5.592 4.55e-08 *** ## gamesyr 0.015759 0.001564 10.079 &lt; 2e-16 *** ## bavg 0.001419 0.001066 1.331 0.184 ## hrunsyr 0.035943 0.007241 4.964 1.08e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7279 on 348 degrees of freedom ## Multiple R-squared: 0.6254, Adjusted R-squared: 0.6211 ## F-statistic: 145.2 on 4 and 348 DF, p-value: &lt; 2.2e-16 ## ## Call: ## lm(formula = lsalary ~ years + gamesyr + bavg + rbisyr, data = mlb1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.97116 -0.45464 -0.05178 0.46468 2.67529 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.128e+01 2.737e-01 41.197 &lt; 2e-16 *** ## years 6.973e-02 1.207e-02 5.776 1.70e-08 *** ## gamesyr 1.116e-02 2.145e-03 5.202 3.37e-07 *** ## bavg 7.398e-04 1.071e-03 0.691 0.49 ## rbisyr 1.652e-02 3.229e-03 5.117 5.13e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7264 on 348 degrees of freedom ## Multiple R-squared: 0.6269, Adjusted R-squared: 0.6226 ## F-statistic: 146.2 on 4 and 348 DF, p-value: &lt; 2.2e-16 dropping rbisyr, in which case hrunsyr becomes very significant. The same is true for rbisyr when hrunsyr is dropped from the model. The F statistic is often useful for testing exclusion of a group of variables when the variables in the group are highly correlated. 4.15 Example 4.9 ## ## Call: ## lm(formula = bwght ~ cigs + parity + faminc + motheduc + fatheduc, ## data = bwght) ## ## Residuals: ## Min 1Q Median 3Q Max ## -95.796 -11.960 0.643 12.679 150.879 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 114.52433 3.72845 30.716 &lt; 2e-16 *** ## cigs -0.59594 0.11035 -5.401 8.02e-08 *** ## parity 1.78760 0.65941 2.711 0.00681 ** ## faminc 0.05604 0.03656 1.533 0.12559 ## motheduc -0.37045 0.31986 -1.158 0.24702 ## fatheduc 0.47239 0.28264 1.671 0.09492 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 19.79 on 1185 degrees of freedom ## Multiple R-squared: 0.03875, Adjusted R-squared: 0.03469 ## F-statistic: 9.553 on 5 and 1185 DF, p-value: 5.986e-09 The R-squared for the full model turns out to be .0387 ## ## Call: ## lm(formula = bwght ~ cigs + parity + faminc, data = bwght) ## ## Residuals: ## Min 1Q Median 3Q Max ## -95.811 -11.552 0.524 12.739 150.848 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 115.46993 1.65590 69.733 &lt; 2e-16 *** ## cigs -0.59785 0.10877 -5.496 4.74e-08 *** ## parity 1.83227 0.65754 2.787 0.00541 ** ## faminc 0.06706 0.03239 2.070 0.03865 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 19.8 on 1187 degrees of freedom ## Multiple R-squared: 0.03642, Adjusted R-squared: 0.03398 ## F-statistic: 14.95 on 3 and 1187 DF, p-value: 1.472e-09 When motheduc and fatheduc are dropped from the regression, the R-squared falls to .0364 ## Analysis of Variance Table ## ## Model 1: bwght ~ cigs + parity + faminc + motheduc + fatheduc ## Model 2: bwght ~ cigs + parity + faminc ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 1185 464041 ## 2 1187 465167 -2 -1125.7 1.4373 0.238 fail to reject H0. In other words, motheduc and fatheduc are jointly insignificant in the birth weight equation. 4.16 Example 4.10 4.17 Problem 1 Which of the following can cause the usual OLS t statistics to be invalid (that is, not to have t distributions under H0)? (i) Heteroskedasticity. A sample correlation coefficient of .95 between two independent variables that are in the model. Omitting an important explanatory variable. and (iii) generally cause the t statistics not to have a t distribution under H0. Homoskedasticity is one of the CLM assumptions. An important omitted variable violates Assumption MLR.3. The CLM assumptions contain no mention of the sample correlations among independent variables, except to rule out the case where the correlation is one. 4.18 Problem 2 4.19 Problem 3 The variable rdintens is expenditures on research and development (R&amp;D) as a percentage of sales. Sales are measured in millions of dollars. The variable profmarg is profits as a percentage of sales. Using the data in RDCHEM for 32 firms in the chemical industry, the following equation is estimated: Using the data in WAGE1 gives the estimated equation \\[rdintens = 0.47225 + 0.32135 lsales + 0.05004 profmarg\\] ## ## Call: ## lm(formula = rdintens ~ lsales + profmarg, data = rdchem) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.3016 -1.2707 -0.6895 0.8785 6.0369 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.47225 1.67606 0.282 0.780 ## lsales 0.32135 0.21557 1.491 0.147 ## profmarg 0.05004 0.04578 1.093 0.283 ## ## Residual standard error: 1.839 on 29 degrees of freedom ## Multiple R-squared: 0.09847, Adjusted R-squared: 0.0363 ## F-statistic: 1.584 on 2 and 29 DF, p-value: 0.2224 Interpret the coefficient on log(sales). In particular, if sales increases by 10%, what is the estimated percentage point change in rdintens? Is this an economically large effect? Holding profmarg fixed, \\(\\Delta\\)rdintens = .321 \\(\\Delta\\)log(sales) = (.321/100)[100 * \\(\\Delta\\)log(sales)] \\(\\sim\\) 0.00321(%\\(\\Delta\\)sales). Therefore, if %\\(\\Delta\\)sales = 10, \\(\\Delta\\)rdintens \\(\\sim\\) .032, or only about 3/100 of a percentage point. For such a large percentage increase in sales, this seems like a practically small effect. Test the hypothesis that R&amp;D intensity does not change with sales against the alternative that it does increase with sales. Do the test at the 5% and 10% levels. \\({H_0}: b_{lsales} = 0\\) \\({H_1}: b_{lsales} &gt; 0\\) where \\(b_{lsales}\\) is the population slope on log(sales). The t statistic is .321/.216 \\(\\sim\\) 1.486. The 5% critical value for a one-tailed test, with df = 32 – 3 = 29, is obtained from Table G.2 as 1.699; so we cannot reject H0 at the 5% level. The 10% critical value is 1.311; since the t statistic is above this value, we reject \\({H_0}\\) in favor of \\({H_1}\\) at the 10% level. ## [1] 1.699127 ## [1] 1.311434 Interpret the coefficient on profmarg. Is it economically large? Does profmarg have a statistically significant effect on rdintens? profmarg is not statistically significant effect on rdintens. 4.20 Problem 5 Consider the estimated equation from Example 4.3, which can be used to study the effects of skipping class on college GPA: ## ## Call: ## lm(formula = colGPA ~ hsGPA + ACT + skipped, data = gpa1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.85698 -0.23200 -0.03935 0.24816 0.81657 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.38955 0.33155 4.191 4.95e-05 *** ## hsGPA 0.41182 0.09367 4.396 2.19e-05 *** ## ACT 0.01472 0.01056 1.393 0.16578 ## skipped -0.08311 0.02600 -3.197 0.00173 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3295 on 137 degrees of freedom ## Multiple R-squared: 0.2336, Adjusted R-squared: 0.2168 ## F-statistic: 13.92 on 3 and 137 DF, p-value: 5.653e-08 Using the standard normal approximation, find the 95% confidence interval for bhsGPA. ## [1] 0.5956486 ## [1] 0.2283514 Can you reject the hypothesis \\({H_0}\\): bhsGPA = .4 against the two-sided alternative at the 5% level? The value bhsGPA = .4 is inside the 95% CI, therefore fail to reject the hypothesis \\({H_0}\\). Can you reject the hypothesis \\({H_0}\\): bhsGPA = 1 against the two-sided alternative at the 5% level? The value bhsGPA = 1 is not inside the 95% CI, therefore reject the hypothesis \\({H_0}\\). 4.21 Problem 7 In Example 4.7, we used data on nonunionized manufacturing firms to estimate the relationship between the scrap rate and other firm characteristics. We now look at this example more closely and use all available firms. The population model estimated in Example 4.7 can be written as \\[lscrap = \\beta_0 + \\beta_1 * hrsemp + \\beta_2 * lsales + \\beta_3 * lemploy + u\\] Using the 43 observations available for 1987, the estimated equation is Compare this equation to that estimated using only the 29 nonunionized firms in the sample. ## ## Call: ## lm(formula = lscrap ~ hrsemp + lsales + lemploy, data = jtrain %&gt;% ## filter(year == 1987)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.81878 -0.91530 0.03304 0.87052 2.68042 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.74426 4.57470 2.567 0.01420 * ## hrsemp -0.04218 0.01868 -2.259 0.02957 * ## lsales -0.95064 0.36984 -2.570 0.01409 * ## lemploy 0.99213 0.35692 2.780 0.00833 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.3 on 39 degrees of freedom ## (114 observations deleted due to missingness) ## Multiple R-squared: 0.3099, Adjusted R-squared: 0.2568 ## F-statistic: 5.838 on 3 and 39 DF, p-value: 0.002148 ## ## Call: ## lm(formula = lscrap ~ hrsemp + lsales + lemploy, data = jtrain %&gt;% ## filter(year == 1987 &amp; union == 0)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.6301 -0.7523 -0.4016 0.8697 2.8273 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.45837 5.68677 2.191 0.0380 * ## hrsemp -0.02927 0.02280 -1.283 0.2111 ## lsales -0.96203 0.45252 -2.126 0.0436 * ## lemploy 0.76147 0.40743 1.869 0.0734 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.376 on 25 degrees of freedom ## (97 observations deleted due to missingness) ## Multiple R-squared: 0.2624, Adjusted R-squared: 0.1739 ## F-statistic: 2.965 on 3 and 25 DF, p-value: 0.05134 ## [1] 0.03894561 ## [1] 0.009645609 ## [1] -0.06824561 The standard error on hrsemp has not changed, the magnitude of the coefficient has increased by half. The t statistic on hrsemp has gone from about –1.28 to –2.21, so now the coefficient is statistically less than zero at the 5% level. (From Table G.2 the 5% critical value with 40 df is –1.684. The 1% critical value is –2.423, so the p-value is between .01 and .05.) Show that the population model can also be written as log(scrap) 5 b0 1 b1hrsemp 1 b2log(sales/employ) 1 u3log(employ) 1 u, where u3 5 b2 1 b3. [Hint: Recall that log(x2/x3) 5 log(x2) 2 log(x3).] Interpret the log(scrap)5 11.74 2 .042 hrsemp2 .951 log(sales)1 .992 log(employ) (4.57) (.019) (.370) (.360) n543,R2 5.310. hypothesis H0: u3 5 0. When the equation from part (ii) is estimated, we obtain Controlling for worker training and for the sales-to-employee ratio, do bigger firms have larger statistically significant scrap rates? Test the hypothesis that a 1% increase in sales/employ is associated with a 1% drop in the scrap rate. 4.22 Problem 9 In Problem 3 in Chapter 3, we estimated the equation \\[sleep = 3638.25 - .148 * totwrk - 11.13 * educ + 2.20 * age \\] \\[n = 706, R^{2} = .113\\] where we now report standard errors along with the estimates. ## ## Call: ## lm(formula = sleep ~ totwrk + educ + age, data = sleep75) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2405.12 -236.50 14.53 253.87 1303.42 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3638.24531 112.27511 32.405 &lt;2e-16 *** ## totwrk -0.14837 0.01669 -8.888 &lt;2e-16 *** ## educ -11.13381 5.88457 -1.892 0.0589 . ## age 2.19988 1.44572 1.522 0.1285 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 419.4 on 702 degrees of freedom ## Multiple R-squared: 0.1134, Adjusted R-squared: 0.1096 ## F-statistic: 29.92 on 3 and 702 DF, p-value: &lt; 2.2e-16 Is either educ or age individually significant at the 5% level against a two-sided alternative? Show your work. With df = 706 – 4 = 702, we use the standard normal critical value (df = \\(\\infty\\) in Table G.2), which is 1.96 for a two-tailed test at the 5% level. \\(t_{educ}\\) = -11.13/5.88 \\(\\sim\\) -1.89, so |\\(t_{educ}\\)| = 1.89 &lt; 1.96, therefore fail to reject \\(H_0\\): \\(t_{educ}\\) = 0 at the 5% level. educ is statistically insignificant at the 5% level. \\(t_{age}\\) = 2.20/1.446 \\(\\sim\\) 1.522, so |\\(t_{age}\\)| = 1.522 &lt; 1.96, therefore fail to reject \\(H_0\\): \\(t_{age}\\) = 0 at the 5% level. age is statistically insignificant at the 5% level. Dropping educ and age from the equation gives Are educ and age jointly significant in the original equation at the 5% level? Justify your answer. ## ## Call: ## lm(formula = sleep ~ totwrk, data = sleep75) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2429.94 -240.25 4.91 250.53 1339.72 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3586.37695 38.91243 92.165 &lt;2e-16 *** ## totwrk -0.15075 0.01674 -9.005 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 421.1 on 704 degrees of freedom ## Multiple R-squared: 0.1033, Adjusted R-squared: 0.102 ## F-statistic: 81.09 on 1 and 704 DF, p-value: &lt; 2.2e-16 Compute the R-squared form of the F statistic for joint significance. ## [1] 3.9891 ## [1] 3.008553 ## Analysis of Variance Table ## ## Model 1: sleep ~ totwrk + educ + age ## Model 2: sleep ~ totwrk ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 702 123455057 ## 2 704 124858119 -2 -1403061 3.9891 0.01894 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 F = (.113 - .103)/(1 - .113) \\(\\sim\\) 3.96. The 5% critical value in the \\(F_{2,702}\\) distribution can be obtained from Table G.3b with denominator df = \\(\\infty\\): critical value = 3.00. Therefore, educ and age are jointly significant at the 5% level (3.96 &gt; 3.00). In fact, the p-value is about .019, and so educ and age are jointly significant at the 5% level. Does including educ and age in the model greatly affect the estimated tradeoff between sleeping and working? These variables are jointly significant, but including them only changes the coefficient on totwrk from -0.151 to -0.148. Suppose that the sleep equation contains heteroskedasticity. What does this mean about the tests computed in parts (i) and (ii)? Bothe standard t and F statistics assume homoskedasticity, in addition to the other CLM assumptions. If the sleep equation contains heteroskedasticity, the tests are not valid. 4.23 Problem 11 The following table was created using the data in CEOSAL, where standard errors are in parentheses below the coefficients: ## ## Call: ## lm(formula = lsalary ~ lsales + lmktval + profmarg + ceoten + ## comten, data = ceosal2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.5436 -0.2796 -0.0164 0.2857 1.9879 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.571977 0.253466 18.038 &lt; 2e-16 *** ## lsales 0.187787 0.040003 4.694 5.46e-06 *** ## lmktval 0.099872 0.049214 2.029 0.04397 * ## profmarg -0.002211 0.002105 -1.050 0.29514 ## ceoten 0.017104 0.005540 3.087 0.00236 ** ## comten -0.009238 0.003337 -2.768 0.00626 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4947 on 171 degrees of freedom ## Multiple R-squared: 0.3525, Adjusted R-squared: 0.3336 ## F-statistic: 18.62 on 5 and 171 DF, p-value: 9.488e-15 ## ## Call: ## lm(formula = lsalary ~ lsales + lmktval + profmarg, data = ceosal2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.26981 -0.30174 -0.01638 0.30474 1.91129 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.620690 0.254344 18.167 &lt; 2e-16 *** ## lsales 0.158483 0.039814 3.981 0.000101 *** ## lmktval 0.112261 0.050393 2.228 0.027190 * ## profmarg -0.002259 0.002165 -1.043 0.298346 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5102 on 173 degrees of freedom ## Multiple R-squared: 0.3035, Adjusted R-squared: 0.2914 ## F-statistic: 25.13 on 3 and 173 DF, p-value: 1.522e-13 ## ## Call: ## lm(formula = lsalary ~ lsales, data = ceosal2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.12794 -0.33918 0.00698 0.30456 1.86858 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.96108 0.19996 24.810 &lt; 2e-16 *** ## lsales 0.22428 0.02713 8.267 3.32e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5154 on 175 degrees of freedom ## Multiple R-squared: 0.2809, Adjusted R-squared: 0.2767 ## F-statistic: 68.35 on 1 and 175 DF, p-value: 3.317e-14 The variable mktval is market value of the firm, profmarg is profit as a percentage of sales, ceoten is years as CEO with the current company, and comten is total years with the company. Comment on the effect of profmarg on CEO In model 1 and 2, the coefficient on lmktval is actually negative (0.0999 and 0.112), although its t statistic is only about –1. Once firm sales and market value have been controlled for, profit margin has no effect on CEO salary. ## Analysis of Variance Table ## ## Model 1: lsalary ~ lsales + lmktval + profmarg + ceoten + comten ## Model 2: lsalary ~ lsales + lmktval + profmarg ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 171 41.856 ## 2 173 45.026 -2 -3.1704 6.4763 0.001944 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Does market value have a significant effect? Explain. In model 1 and 2, the coefficient on profmarg is positive (-0.00221 and -0.00226), although its t statistic is only about 0.1. Once firm sales and profit as a percentage of sales have been controlled for, market value haseffect on CEO salary. In model 1, market value controls for the most factors affecting salary. The t statistic on log(mktval) is 2.03, which is significant at the 5% level against a two-sided alternative. log(mktval) is statistically significant. Because the coefficient is an elasticity, a ceteris paribus 10% increase in market value is predicted to increase salary by 1%. This is not a large nor negligible effect. Interpret the coefficients on ceoten and comten. Are these explanatory variables statistically significant? In model 1, years as ceo with company the t statistic on ceoten is 3.09, which is significant at the 5% level against a two-sided alternative. ceoten is statistically significant. Other factors fixed, another year as CEO with the company increases salary by about 1.71%. In model 1, years with company the t statistic on ceoten is -2.77, which is significant at the 5% level against a two-sided alternative. comten is statistically significant. Other factors fixed, another year with the company decreases salary by about 1.71%. What do you make of the fact that longer tenure with the company, holding the other factors fixed, is associated with a lower salary? One possibility is that firms that hire CEOs from outside the company (lower tenure with the company) often go after a small pool of highly regarded candidates, and salaries of these people are bid up. 4.24 C1 The following model can be used to study whether campaign expenditures affect election outcomes: \\[voteA = \\beta_0 + \\beta_1 * lexpendA + \\beta_2 * lexpendB + \\beta_3 * prtystrA + u\\] where voteA is the percentage of the vote received by Candidate A, expendA and expendB are campaign expenditures by Candidates A and b, and prtystrA is a measure of party strength for Candidate A (the percentage of the most recent presidential vote that went to A’s party). What is the interpretation of \\(\\beta_1\\)? ## ## Call: ## lm(formula = voteA ~ lexpendA + lexpendB + prtystrA, data = vote1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.3968 -5.4174 -0.8679 4.9551 26.0660 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 45.07893 3.92631 11.48 &lt;2e-16 *** ## lexpendA 6.08332 0.38215 15.92 &lt;2e-16 *** ## lexpendB -6.61542 0.37882 -17.46 &lt;2e-16 *** ## prtystrA 0.15196 0.06202 2.45 0.0153 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.712 on 169 degrees of freedom ## Multiple R-squared: 0.7926, Adjusted R-squared: 0.7889 ## F-statistic: 215.2 on 3 and 169 DF, p-value: &lt; 2.2e-16 Holding other factors fixed, \\(\\Delta\\)voteA = \\(\\beta_1\\Delta\\)lexpendA where we use the fact that 100 * \\(\\Delta\\)lexpendA \\(\\sim\\) %\\(\\Delta\\)lexpendA. So \\(\\beta_1\\)/100 is the (ceteris paribus) percentage point change in voteA when expendA increases by one percent. In terms of the parameters, state the null hypothesis that a 1% increase in A’s expenditures is offset by a 1% increase in B’s expenditures. \\({H_0}: \\beta_1 - \\beta_2 = 0\\) \\({H_1}: \\beta_1 - \\beta_2 \\ne 0\\) Estimate the given model using the data in VOTE1 and report the results in usual form. Do A’s expenditures affect the outcome? What about B’s expenditures? Can you use these results to test the hypothesis in part (ii)? Estimate a model that directly gives the t statistic for testing the hypothesis in part What do you conclude? (Use a two-sided alternative.) The coefficient on log(expendA) is very significant (t statistic = 15.9). The estimates imply that a 10% ceteris paribus increase in spending by candidate A increases the predicted share of the vote going to A by about 0.608 percentage points. The coefficient on log(expendB) is very significant (t statistic = -17.5). The estimates imply that a 10% ceteris paribus decrease in spending by candidate B increases the predicted share of the vote going to A by about -0.662 percentage points. The coefficients on log(expendA) and log(expendB) are of similar magnitudes but opposite in sign, we do not have the standard error of + , which is what we would need to test the hypothesis from part (ii). 4.25 C2 Refer to Computer Exercise C2 in Chapter 3. Now, use the log of the housing price as the dependent variable: \\[log(price) = \\beta_0 + \\beta_1 * sqrft + \\beta_2 * bdrms + u\\] You are interested in estimating and obtaining a confidence interval for the percentage change in price when a 150-square-foot bedroom is added to a house. In decimal form, this is \\(\\theta_1\\) = 150 \\(\\beta_1\\) + \\(\\beta_2\\) Use the data in HPRICE1 to estimate u1. ## ## Call: ## lm(formula = lprice ~ sqrft + bdrms, data = hprice1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.75448 -0.12322 -0.01993 0.11938 0.62948 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.766e+00 9.704e-02 49.112 &lt; 2e-16 *** ## sqrft 3.794e-04 4.321e-05 8.781 1.5e-13 *** ## bdrms 2.888e-02 2.964e-02 0.974 0.333 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1971 on 85 degrees of freedom ## Multiple R-squared: 0.5883, Adjusted R-squared: 0.5786 ## F-statistic: 60.73 on 2 and 85 DF, p-value: &lt; 2.2e-16 Therefore, \\(\\hat{\\theta_1}\\) = 150 (0.000379) + 0.0289 = 0.0858, which means that an additional 150 square foot bedroom increases the predicted price by about 8.6%. \\(\\hat{\\theta_1}\\) = 0.0858, \\(\\beta_1\\) = 0.000379, \\(\\beta_2\\) = 0.0289 Write \\(\\beta_2\\) in terms of \\(\\theta_1\\) and \\(\\beta_1\\) and plug this into the log(price) equation. \\(\\theta_1\\) = 150 \\(\\beta_1\\) + \\(\\beta_2\\) \\(\\theta_1\\) - 150 \\(\\beta_1\\) = \\(\\beta_2\\) log(price) = \\(\\beta_0\\) + \\(\\beta_1\\) sqrft + (\\(\\theta_1\\) – 150 \\(\\beta_1\\)) bdrms + u log(price) = \\(\\beta_0\\) + \\(\\beta_1\\) (sqrft – 150 bdrms) + \\(\\theta_1\\) bdrms + u. Use part (ii) to obtain a standard error for \\(\\theta_1\\) and use this standard error to construct a 95% confidence interval. log(price) on (sqrft – 150 bdrms) \\(\\hat{\\theta_1}\\) = 0.0858; now we also get se(\\(\\hat{\\theta_1}\\)) = 0.0268. The 95% confidence interval is 0.0326 to 0.1390 (3.3% to 13.9%). "],
["multiple-regression-analysis-ols-asymptotics.html", "Chapter 5 Multiple regression analysis: OLS asymptotics 5.1 Problem 3 5.2 C1 5.3 C2 5.4 C3 5.5 C5", " Chapter 5 Multiple regression analysis: OLS asymptotics 5.1 Problem 3 The data set SMOKE contains information on smoking behavior and other variables for a random sample of single adults from the United States. The variable cigs is the (average) number of cigarettes smoked per day. Do you think cigs has a normal distribution in the U.S. adult population? Explain. 5.2 C1 Use the data in WAGE1.RAW for this exercise. (i) Estimate the equation \\[wage = \\beta_0 + \\beta_1 * educ + \\beta_2 * exper + \\beta_3 * tenure + u\\] Save the residuals and plot a histogram. ## ## Call: ## lm(formula = wage ~ educ + exper + tenure, data = wage1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.6068 -1.7747 -0.6279 1.1969 14.6536 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.87273 0.72896 -3.941 9.22e-05 *** ## educ 0.59897 0.05128 11.679 &lt; 2e-16 *** ## exper 0.02234 0.01206 1.853 0.0645 . ## tenure 0.16927 0.02164 7.820 2.93e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.084 on 522 degrees of freedom ## Multiple R-squared: 0.3064, Adjusted R-squared: 0.3024 ## F-statistic: 76.87 on 3 and 522 DF, p-value: &lt; 2.2e-16 ## [[1]] ## ## [[2]] ## ## [[3]] ## ## [[4]] Repeat part (i), but with log(wage) as the dependent variable. ## ## Call: ## lm(formula = lwage ~ educ + exper + tenure, data = wage1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.05802 -0.29645 -0.03265 0.28788 1.42809 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.284360 0.104190 2.729 0.00656 ** ## educ 0.092029 0.007330 12.555 &lt; 2e-16 *** ## exper 0.004121 0.001723 2.391 0.01714 * ## tenure 0.022067 0.003094 7.133 3.29e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4409 on 522 degrees of freedom ## Multiple R-squared: 0.316, Adjusted R-squared: 0.3121 ## F-statistic: 80.39 on 3 and 522 DF, p-value: &lt; 2.2e-16 ## [[1]] ## ## [[2]] ## ## [[3]] ## ## [[4]] Would you say that Assumption MLR.6 is closer to being satisfied for the level-level model or the log-level model? Assumption MLR.6 is closer to being satisfied for the log-level model. 5.3 C2 Use the data in GPA2 for this exercise. Using all 4,137 observations, estimate the equation \\[colgpa = \\beta_0 + \\beta_1 * hsperc + \\beta_2 * sat + u\\] and report the results in standard form. ## ## Call: ## lm(formula = colgpa ~ hsperc + sat, data = gpa2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.6007 -0.3581 0.0329 0.3963 1.7599 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.392e+00 7.154e-02 19.45 &lt;2e-16 *** ## hsperc -1.352e-02 5.495e-04 -24.60 &lt;2e-16 *** ## sat 1.476e-03 6.531e-05 22.60 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5615 on 4134 degrees of freedom ## Multiple R-squared: 0.2734, Adjusted R-squared: 0.2731 ## F-statistic: 777.9 on 2 and 4134 DF, p-value: &lt; 2.2e-16 Reestimate the equation in part (i), using the first 2,070 observations. ## ## Call: ## lm(formula = colgpa ~ hsperc + sat, data = gpa2[1:2070, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.28027 -0.34910 0.04051 0.38046 1.69464 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.436e+00 9.778e-02 14.69 &lt;2e-16 *** ## hsperc -1.275e-02 7.185e-04 -17.74 &lt;2e-16 *** ## sat 1.468e-03 8.858e-05 16.58 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5395 on 2067 degrees of freedom ## Multiple R-squared: 0.2827, Adjusted R-squared: 0.282 ## F-statistic: 407.4 on 2 and 2067 DF, p-value: &lt; 2.2e-16 Find the ratio of the standard errors on hsperc from parts (i) and (ii). Compare this with the result from (5.10). 5.4 C3 In equation (4.42) of Chapter 4, using the data set BWGHT, compute the LM statistic for testing whether motheduc and fatheduc are jointly significant. In obtaining the residuals for the restricted model, be sure that the restricted model is estimated using only those observations for which all variables in the unrestricted model are available (see Example 4.9). ## ## Call: ## lm(formula = bwght ~ cigs + parity + faminc + motheduc + fatheduc, ## data = bwght) ## ## Residuals: ## Min 1Q Median 3Q Max ## -95.796 -11.960 0.643 12.679 150.879 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 114.52433 3.72845 30.716 &lt; 2e-16 *** ## cigs -0.59594 0.11035 -5.401 8.02e-08 *** ## parity 1.78760 0.65941 2.711 0.00681 ** ## faminc 0.05604 0.03656 1.533 0.12559 ## motheduc -0.37045 0.31986 -1.158 0.24702 ## fatheduc 0.47239 0.28264 1.671 0.09492 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 19.79 on 1185 degrees of freedom ## Multiple R-squared: 0.03875, Adjusted R-squared: 0.03469 ## F-statistic: 9.553 on 5 and 1185 DF, p-value: 5.986e-09 ## [1] faminc cigtax cigprice bwght fatheduc motheduc parity ## [8] male white cigs lbwght bwghtlbs packs lfaminc ## &lt;0 rows&gt; (or 0-length row.names) ## ## Call: ## lm(formula = bwght ~ cigs + parity + faminc, data = bwght) ## ## Residuals: ## Min 1Q Median 3Q Max ## -95.811 -11.552 0.524 12.739 150.848 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 115.46993 1.65590 69.733 &lt; 2e-16 *** ## cigs -0.59785 0.10877 -5.496 4.74e-08 *** ## parity 1.83227 0.65754 2.787 0.00541 ** ## faminc 0.06706 0.03239 2.070 0.03865 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 19.8 on 1187 degrees of freedom ## Multiple R-squared: 0.03642, Adjusted R-squared: 0.03398 ## F-statistic: 14.95 on 3 and 1187 DF, p-value: 1.472e-09 ## Analysis of Variance Table ## ## Model 1: bwght ~ cigs + parity + faminc + motheduc + fatheduc ## Model 2: bwght ~ cigs + parity + faminc ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 1185 464041 ## 2 1187 465167 -2 -1125.7 1.4373 0.238 5.5 C5 Consider the analysis in Computer Exercise C11 in Chapter 4 using the data in HTV, where educ is the dependent variable in a regression. educ 5 0 1 1motheduc 1 2 fatheduc 1 3abil 1 4abil2 1 u ## ## Call: ## lm(formula = educ ~ motheduc + fatheduc + abil + abil2, data = htv) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.2506 -1.1274 -0.1355 1.0223 7.0482 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.240226 0.287410 28.671 &lt; 2e-16 *** ## motheduc 0.190126 0.028096 6.767 2.03e-11 *** ## fatheduc 0.108939 0.019601 5.558 3.35e-08 *** ## abil 0.401462 0.030288 13.255 &lt; 2e-16 *** ## abil2 0.050599 0.008304 6.093 1.48e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.758 on 1225 degrees of freedom ## Multiple R-squared: 0.4444, Adjusted R-squared: 0.4425 ## F-statistic: 244.9 on 4 and 1225 DF, p-value: &lt; 2.2e-16 How many different values are taken on by educ in the sample? Does educ have a continuous distribution? There are 15 different values are taken on by educ in the sample. educ does not have a continuous distribution. ## # A tibble: 15 x 2 ## educ n ## &lt;fct&gt; &lt;int&gt; ## 1 6 2 ## 2 7 3 ## 3 8 29 ## 4 9 41 ## 5 10 47 ## 6 11 64 ## 7 12 512 ## 8 13 93 ## 9 14 67 ## 10 15 206 ## 11 16 70 ## 12 17 47 ## 13 18 19 ## 14 19 14 ## 15 20 16 Plot a histogram of educ with a normal distribution overlay. Does the distribution of educ appear anything close to normal? Which of the CLM assumptions seems clearly violated in the model? \\[educ = \\beta_0 + \\beta_1 * motheduc + \\beta_2 * fatheduc + \\beta_3 * abil + \\beta_4 * abil^2 + u\\], The homoscedasticity assumptions is clearly violated in the model. How does this violation change the statistical inference procedures carried out in Computer Exercise C11 in Chapter 4? ## [[1]] ## ## [[2]] ## ## [[3]] ## ## [[4]] "],
["multiple-regression-analysis-further-issues.html", "Chapter 6 Multiple Regression Analysis: Further Issues 6.1 Effects of Data Scaling on OLS Statistics 6.2 Example 1 6.3 Example 2 6.4 Example 3 6.5 Example 4 6.6 Example 5 6.7 Example 3 6.8 Explore 6.3 6.9 Explore 6.5", " Chapter 6 Multiple Regression Analysis: Further Issues 6.1 Effects of Data Scaling on OLS Statistics ## ## Call: ## lm(formula = bwght ~ cigs + faminc, data = bwght) ## ## Residuals: ## Min 1Q Median 3Q Max ## -96.061 -11.543 0.638 13.126 150.083 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 116.97413 1.04898 111.512 &lt; 2e-16 *** ## cigs -0.46341 0.09158 -5.060 4.75e-07 *** ## faminc 0.09276 0.02919 3.178 0.00151 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20.06 on 1385 degrees of freedom ## Multiple R-squared: 0.0298, Adjusted R-squared: 0.0284 ## F-statistic: 21.27 on 2 and 1385 DF, p-value: 7.942e-10 ## ## Call: ## lm(formula = bwghtlbs ~ cigs + faminc, data = bwght) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.0038 -0.7215 0.0399 0.8204 9.3802 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.310883 0.065562 111.512 &lt; 2e-16 *** ## cigs -0.028963 0.005724 -5.060 4.75e-07 *** ## faminc 0.005798 0.001824 3.178 0.00151 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.254 on 1385 degrees of freedom ## Multiple R-squared: 0.0298, Adjusted R-squared: 0.0284 ## F-statistic: 21.27 on 2 and 1385 DF, p-value: 7.942e-10 ## ## Call: ## lm(formula = bwght ~ packs + faminc, data = bwght) ## ## Residuals: ## Min 1Q Median 3Q Max ## -96.061 -11.543 0.638 13.126 150.083 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 116.97413 1.04898 111.512 &lt; 2e-16 *** ## packs -9.26815 1.83154 -5.060 4.75e-07 *** ## faminc 0.09276 0.02919 3.178 0.00151 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20.06 on 1385 degrees of freedom ## Multiple R-squared: 0.0298, Adjusted R-squared: 0.0284 ## F-statistic: 21.27 on 2 and 1385 DF, p-value: 7.942e-10 Once the effects are transformed into the same units, we get exactly the same answer, regardless of how the dependent variable is measured. Changing the dependent variable from ounces to pounds has no effect on how statistically important the independent variables are. The standard errors in model 2 are 16 times smaller than those in model 1 (t statistics are indeed identical). SER in model 2 is 16 times smaller than that in model 1. In terms of goodness-of-fit, the R-squareds from the two regressions are identical. 6.2 Example 1 We use the data from Example 4.5 (in the file HPRICE2) to illustrate the use of beta coefficients. Recall that the key independent variable is nox, a measure of the nitrogen oxide in the air over each community. One way to understand the size of the pollution effect—without getting into the science underlying nitrogen oxide’s effect on air quality—is to compute beta coefficients. (An alternative approach is contained in Example 4.5: we obtained a price elasticity with respect to nox by using price and nox in logarithmic form.) ## ## Call: ## lm(formula = price ~ nox + crime + rooms + dist + stratio, data = hprice2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13914 -3201 -662 2110 38064 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20871.13 5054.60 4.129 4.27e-05 *** ## nox -2706.43 354.09 -7.643 1.09e-13 *** ## crime -153.60 32.93 -4.665 3.97e-06 *** ## rooms 6735.50 393.60 17.112 &lt; 2e-16 *** ## dist -1026.81 188.11 -5.459 7.57e-08 *** ## stratio -1149.20 127.43 -9.018 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5586 on 500 degrees of freedom ## Multiple R-squared: 0.6357, Adjusted R-squared: 0.632 ## F-statistic: 174.5 on 5 and 500 DF, p-value: &lt; 2.2e-16 ## ## Call: ## lm(formula = zprice ~ znox + zcrime + zrooms + zdist + zstratio, ## data = hprice2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.5110 -0.3476 -0.0719 0.2291 4.1334 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.679e-16 2.697e-02 0.000 1 ## znox -3.404e-01 4.454e-02 -7.643 1.09e-13 *** ## zcrime -1.433e-01 3.072e-02 -4.665 3.97e-06 *** ## zrooms 5.139e-01 3.003e-02 17.112 &lt; 2e-16 *** ## zdist -2.348e-01 4.302e-02 -5.459 7.57e-08 *** ## zstratio -2.703e-01 2.997e-02 -9.018 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6066 on 500 degrees of freedom ## Multiple R-squared: 0.6357, Adjusted R-squared: 0.632 ## F-statistic: 174.5 on 5 and 500 DF, p-value: &lt; 2.2e-16 This equation shows that a one standard deviation increase in nox decreases price by .34 standard deviation; a one standard deviation increase in crime reduces price by .14 standard deviation. Thus, the same relative movement of pollution in the population has a larger effect on housing prices than crime does. Size of the house, as measured by number of rooms (rooms), has the largest standardized effect. If we want to know the effects of each independent variable on the dollar value of median house price, we should use the unstandardized variables. Whether we use standardized or unstandardized variables does not affect statistical significance: the t statistics are the same in both cases. ## ## Call: ## lm(formula = lprice ~ lnox + rooms, data = hprice2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.06485 -0.12331 0.00782 0.14471 1.38770 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.23374 0.18774 49.18 &lt;2e-16 *** ## lnox -0.71767 0.06634 -10.82 &lt;2e-16 *** ## rooms 0.30592 0.01902 16.09 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.286 on 503 degrees of freedom ## Multiple R-squared: 0.5137, Adjusted R-squared: 0.5118 ## F-statistic: 265.7 on 2 and 503 DF, p-value: &lt; 2.2e-16 nox increases by 1%, price falls by .718%, holding only rooms fixed rooms increases by one, price increases by approximately 100(.306) 5 30.6% 100[exp(.306) + 1] = 35.8%, which is larger than the approximate percentage change as the change in log(y) becomes larger and larger, the approximation becomes more and more inaccurate ## ## Call: ## lm(formula = wage ~ exper + exper2, data = wage1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.5916 -2.1440 -0.8603 1.1801 17.7649 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.7254058 0.3459392 10.769 &lt; 2e-16 *** ## exper 0.2981001 0.0409655 7.277 1.26e-12 *** ## exper2 -0.0061299 0.0009025 -6.792 3.02e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.524 on 523 degrees of freedom ## Multiple R-squared: 0.09277, Adjusted R-squared: 0.0893 ## F-statistic: 26.74 on 2 and 523 DF, p-value: 8.774e-12 6.3 Example 2 ## ## Call: ## lm(formula = lprice ~ lnox + log(dist) + rooms + I(rooms^2) + ## stratio, data = hprice2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.04285 -0.12774 0.02038 0.12650 1.25272 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.385478 0.566473 23.630 &lt; 2e-16 *** ## lnox -0.901682 0.114687 -7.862 2.34e-14 *** ## log(dist) -0.086781 0.043281 -2.005 0.04549 * ## rooms -0.545113 0.165454 -3.295 0.00106 ** ## I(rooms^2) 0.062261 0.012805 4.862 1.56e-06 *** ## stratio -0.047590 0.005854 -8.129 3.42e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2592 on 500 degrees of freedom ## Multiple R-squared: 0.6028, Adjusted R-squared: 0.5988 ## F-statistic: 151.8 on 5 and 500 DF, p-value: &lt; 2.2e-16 ## ## Call: ## lm(formula = lprice ~ lnox + I(lnox^2) + crime + rooms + I(rooms^2) + ## stratio, data = hprice2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.87229 -0.10976 0.00203 0.10383 1.22275 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.811804 0.893758 12.097 &lt; 2e-16 *** ## lnox 2.438673 0.902243 2.703 0.00711 ** ## I(lnox^2) -0.844556 0.259702 -3.252 0.00122 ** ## crime -0.014247 0.001387 -10.273 &lt; 2e-16 *** ## rooms -0.783909 0.146791 -5.340 1.41e-07 *** ## I(rooms^2) 0.079962 0.011348 7.047 6.13e-12 *** ## stratio -0.041385 0.005790 -7.147 3.16e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2339 on 499 degrees of freedom ## Multiple R-squared: 0.6771, Adjusted R-squared: 0.6732 ## F-statistic: 174.4 on 6 and 499 DF, p-value: &lt; 2.2e-16 6.4 Example 3 ## ## Call: ## lm(formula = stndfnl ~ atndrte + priGPA + ACT + I(priGPA^2) + ## I(ACT^2) + I(atndrte * priGPA), data = attend) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.1698 -0.5316 -0.0177 0.5737 2.3344 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.050293 1.360319 1.507 0.132225 ## atndrte -0.006713 0.010232 -0.656 0.512005 ## priGPA -1.628540 0.481003 -3.386 0.000751 *** ## ACT -0.128039 0.098492 -1.300 0.194047 ## I(priGPA^2) 0.295905 0.101049 2.928 0.003523 ** ## I(ACT^2) 0.004533 0.002176 2.083 0.037634 * ## I(atndrte * priGPA) 0.005586 0.004317 1.294 0.196173 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8729 on 673 degrees of freedom ## Multiple R-squared: 0.2287, Adjusted R-squared: 0.2218 ## F-statistic: 33.25 on 6 and 673 DF, p-value: &lt; 2.2e-16 ## ## Call: ## lm(formula = lsalary ~ years + gamesyr + bavg + hrunsyr, data = mlb1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.0642 -0.4614 -0.0271 0.4654 2.7216 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.020912 0.265719 41.476 &lt; 2e-16 *** ## years 0.067732 0.012113 5.592 4.55e-08 *** ## gamesyr 0.015759 0.001564 10.079 &lt; 2e-16 *** ## bavg 0.001419 0.001066 1.331 0.184 ## hrunsyr 0.035943 0.007241 4.964 1.08e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7279 on 348 degrees of freedom ## Multiple R-squared: 0.6254, Adjusted R-squared: 0.6211 ## F-statistic: 145.2 on 4 and 348 DF, p-value: &lt; 2.2e-16 ## ## Call: ## lm(formula = lsalary ~ years + gamesyr + bavg + rbisyr, data = mlb1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.97116 -0.45464 -0.05178 0.46468 2.67529 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.128e+01 2.737e-01 41.197 &lt; 2e-16 *** ## years 6.973e-02 1.207e-02 5.776 1.70e-08 *** ## gamesyr 1.116e-02 2.145e-03 5.202 3.37e-07 *** ## bavg 7.398e-04 1.071e-03 0.691 0.49 ## rbisyr 1.652e-02 3.229e-03 5.117 5.13e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7264 on 348 degrees of freedom ## Multiple R-squared: 0.6269, Adjusted R-squared: 0.6226 ## F-statistic: 146.2 on 4 and 348 DF, p-value: &lt; 2.2e-16 ## ## Call: ## lm(formula = rdintens ~ lsales, data = rdchem) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.9710 -1.4323 -0.5639 0.9874 5.7748 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.1043 1.5782 0.7 0.490 ## lsales 0.3017 0.2155 1.4 0.172 ## ## Residual standard error: 1.845 on 30 degrees of freedom ## Multiple R-squared: 0.06133, Adjusted R-squared: 0.03004 ## F-statistic: 1.96 on 1 and 30 DF, p-value: 0.1718 ## ## Call: ## lm(formula = rdintens ~ sales + I(sales^2), data = rdchem) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.1418 -1.3630 -0.2257 1.0688 5.5808 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.613e+00 4.294e-01 6.084 1.27e-06 *** ## sales 3.006e-04 1.393e-04 2.158 0.0394 * ## I(sales^2) -6.946e-09 3.726e-09 -1.864 0.0725 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.788 on 29 degrees of freedom ## Multiple R-squared: 0.1484, Adjusted R-squared: 0.08969 ## F-statistic: 2.527 on 2 and 29 DF, p-value: 0.09733 6.5 Example 4 ## ## Call: ## lm(formula = salary ~ sales + roe, data = ceosal1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1501.8 -492.6 -232.0 123.3 13575.2 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.306e+02 2.239e+02 3.710 0.000267 *** ## sales 1.634e-02 8.874e-03 1.842 0.066973 . ## roe 1.963e+01 1.108e+01 1.772 0.077823 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1359 on 206 degrees of freedom ## Multiple R-squared: 0.02917, Adjusted R-squared: 0.01975 ## F-statistic: 3.095 on 2 and 206 DF, p-value: 0.04739 ## ## Call: ## lm(formula = lsalary ~ lsales + roe, data = ceosal1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.9464 -0.2888 -0.0322 0.2261 2.7830 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.362167 0.293878 14.843 &lt; 2e-16 *** ## lsales 0.275087 0.033254 8.272 1.62e-14 *** ## roe 0.017872 0.003955 4.519 1.05e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4822 on 206 degrees of freedom ## Multiple R-squared: 0.282, Adjusted R-squared: 0.275 ## F-statistic: 40.45 on 2 and 206 DF, p-value: 1.519e-15 6.6 Example 5 ## ## Call: ## lm(formula = colgpa ~ sat + hsperc + hsize + hsizesq, data = gpa2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.57543 -0.35081 0.03342 0.39945 1.81683 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.493e+00 7.534e-02 19.812 &lt; 2e-16 *** ## sat 1.492e-03 6.521e-05 22.886 &lt; 2e-16 *** ## hsperc -1.386e-02 5.610e-04 -24.698 &lt; 2e-16 *** ## hsize -6.088e-02 1.650e-02 -3.690 0.000228 *** ## hsizesq 5.460e-03 2.270e-03 2.406 0.016191 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5599 on 4132 degrees of freedom ## Multiple R-squared: 0.2781, Adjusted R-squared: 0.2774 ## F-statistic: 398 on 4 and 4132 DF, p-value: &lt; 2.2e-16 ## $fit ## 1 ## 2.700075 ## ## $se.fit ## [1] 0.01987784 ## ## $df ## [1] 4132 ## ## $residual.scale ## [1] 0.5598638 ## ## Call: ## lm(formula = colgpa ~ sat0 + hsperc0 + hsize0 + hsizesq0, data = gpa2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.57543 -0.35081 0.03342 0.39945 1.81683 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.700e+00 1.988e-02 135.833 &lt; 2e-16 *** ## sat0 1.492e-03 6.521e-05 22.886 &lt; 2e-16 *** ## hsperc0 -1.386e-02 5.610e-04 -24.698 &lt; 2e-16 *** ## hsize0 -6.088e-02 1.650e-02 -3.690 0.000228 *** ## hsizesq0 5.460e-03 2.270e-03 2.406 0.016191 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5599 on 4132 degrees of freedom ## Multiple R-squared: 0.2781, Adjusted R-squared: 0.2774 ## F-statistic: 398 on 4 and 4132 DF, p-value: &lt; 2.2e-16 6.7 Example 3 ## ## Call: ## lm(formula = lsalary ~ lsales + lmktval + ceoten, data = ceosal2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.49693 -0.29472 0.00964 0.30417 1.85286 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.503795 0.257234 17.509 &lt; 2e-16 *** ## lsales 0.162854 0.039242 4.150 5.21e-05 *** ## lmktval 0.109243 0.049595 2.203 0.0289 * ## ceoten 0.011705 0.005326 2.198 0.0293 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5048 on 173 degrees of freedom ## Multiple R-squared: 0.3182, Adjusted R-squared: 0.3063 ## F-statistic: 26.91 on 3 and 173 DF, p-value: 2.474e-14 ## 1 ## 1112.18 6.8 Explore 6.3 If we add the term $_74 ACT * atndrte to equation (6.18), what is the partial effect of atndrte on stndfnl? ## ## Call: ## lm(formula = stndfnl ~ atndrte + priGPA + ACT + I(priGPA^2) + ## I(ACT^2) + I(atndrte * priGPA) + I(ACT * atndrte), data = attend) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.16053 -0.53800 -0.01647 0.56182 2.33417 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.1697205 1.9086655 2.185 0.02926 * ## atndrte -0.0286160 0.0172147 -1.662 0.09692 . ## priGPA -1.5216291 0.4852014 -3.136 0.00179 ** ## ACT -0.2435398 0.1225350 -1.988 0.04727 * ## I(priGPA^2) 0.2784251 0.1015406 2.742 0.00627 ** ## I(ACT^2) 0.0053608 0.0022361 2.397 0.01678 * ## I(atndrte * priGPA) 0.0052925 0.0043166 1.226 0.22059 ## I(ACT * atndrte) 0.0009561 0.0006046 1.581 0.11430 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8719 on 672 degrees of freedom ## Multiple R-squared: 0.2315, Adjusted R-squared: 0.2235 ## F-statistic: 28.92 on 7 and 672 DF, p-value: &lt; 2.2e-16 6.9 Explore 6.5 How would you use residual analysis to deter- mine which professional athletes are overpaid or underpaid relative to their performance? "],
["multiple-regression-analysis-with-qualitative-information-binary-or-dummy-variables.html", "Chapter 7 Multiple Regression Analysis with Qualitative Information: Binary (or Dummy) Variables 7.1 Example 1 7.2 Example 2 7.3 Example 3", " Chapter 7 Multiple Regression Analysis with Qualitative Information: Binary (or Dummy) Variables Suppose that, in a study comparing election outcomes between Democratic and Republican candidates, you wish to indi- cate the party of each candidate. Is a name such as party a wise choice for a binary variable in this case? What would be a better name? 7.1 Example 1 ## ## Call: ## lm(formula = wage ~ educ + female, data = wage1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.9890 -1.8702 -0.6651 1.0447 15.4998 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.62282 0.67253 0.926 0.355 ## educ 0.50645 0.05039 10.051 &lt; 2e-16 *** ## female1 -2.27336 0.27904 -8.147 2.76e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.186 on 523 degrees of freedom ## Multiple R-squared: 0.2588, Adjusted R-squared: 0.256 ## F-statistic: 91.32 on 2 and 523 DF, p-value: &lt; 2.2e-16 ## ## Call: ## lm(formula = wage ~ educ + female + exper + tenure, data = wage1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.7675 -1.8080 -0.4229 1.0467 14.0075 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.56794 0.72455 -2.164 0.0309 * ## educ 0.57150 0.04934 11.584 &lt; 2e-16 *** ## female1 -1.81085 0.26483 -6.838 2.26e-11 *** ## exper 0.02540 0.01157 2.195 0.0286 * ## tenure 0.14101 0.02116 6.663 6.83e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.958 on 521 degrees of freedom ## Multiple R-squared: 0.3635, Adjusted R-squared: 0.3587 ## F-statistic: 74.4 on 4 and 521 DF, p-value: &lt; 2.2e-16 The negative intercept—the intercept for men, in this case—is not very meaningful because no one has zero values for all of educ, exper, and tenure. If we take a woman and a man with the same levels of education, experience, and tenure, the woman earns, on average, $1.81 less per hour than the man. ## ## Call: ## lm(formula = wage ~ female, data = wage1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.5995 -1.8495 -0.9877 1.4260 17.8805 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.0995 0.2100 33.806 &lt; 2e-16 *** ## female1 -2.5118 0.3034 -8.279 1.04e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.476 on 524 degrees of freedom ## Multiple R-squared: 0.1157, Adjusted R-squared: 0.114 ## F-statistic: 68.54 on 1 and 524 DF, p-value: 1.042e-15 men earn $7.10 per hour on average average wage for women in the sample is 7.10 - 2.51 = 4.59, or $4.59 per hour The estimated wage differential between men and women is larger in model 2 than in model 1 because model 2 does not control for differences in education, experience, and tenure, and these are lower, on average, for women than for men 7.2 Example 2 ## ## Call: ## lm(formula = colGPA ~ PC + hsGPA + ACT, data = gpa1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.7901 -0.2622 -0.0107 0.2334 0.7570 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.263520 0.333125 3.793 0.000223 *** ## PC1 0.157309 0.057287 2.746 0.006844 ** ## hsGPA 0.447242 0.093647 4.776 4.54e-06 *** ## ACT 0.008659 0.010534 0.822 0.412513 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3325 on 137 degrees of freedom ## Multiple R-squared: 0.2194, Adjusted R-squared: 0.2023 ## F-statistic: 12.83 on 3 and 137 DF, p-value: 1.932e-07 ## ## Call: ## lm(formula = colGPA ~ PC, data = gpa1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.95893 -0.25893 0.01059 0.31059 0.84107 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.98941 0.03950 75.678 &lt;2e-16 *** ## PC1 0.16952 0.06268 2.704 0.0077 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3642 on 139 degrees of freedom ## Multiple R-squared: 0.04999, Adjusted R-squared: 0.04315 ## F-statistic: 7.314 on 1 and 139 DF, p-value: 0.007697 student who owns a PC has a predicted GPA about .16 points higher than a comparable student without a PC Regressing colGPA on PC gives an estimate on PC equal to about .17. 7.3 Example 3 ## ## Call: ## lm(formula = hrsemp ~ grant + lsales + lemploy, data = jtrain %&gt;% ## filter(year == 1988)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -36.874 -13.128 -3.642 4.770 119.618 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 46.6651 43.4121 1.075 0.285 ## grant 26.2545 5.5918 4.695 8.4e-06 *** ## lsales -0.9846 3.5399 -0.278 0.781 ## lemploy -6.0699 3.8829 -1.563 0.121 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 24.38 on 101 degrees of freedom ## (52 observations deleted due to missingness) ## Multiple R-squared: 0.2368, Adjusted R-squared: 0.2141 ## F-statistic: 10.44 on 3 and 101 DF, p-value: 4.804e-06 cannot enter hrsemp in logarithmic form because hrsemp is zero for 29 grant is very statistically significant coefficient on log(sales) is small and very insignificant coefficient on log(employ) means that, if a firm is 10% larger, it trains its workers about .61 hour less. Its t statistic is 21.56, which is only marginally statistically significan ## ## Call: ## lm(formula = lprice ~ llotsize + lsqrft + bdrms + colonial, data = hprice1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.69479 -0.09750 -0.01619 0.09151 0.70228 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.34959 0.65104 -2.073 0.0413 * ## llotsize 0.16782 0.03818 4.395 3.25e-05 *** ## lsqrft 0.70719 0.09280 7.620 3.69e-11 *** ## bdrms 0.02683 0.02872 0.934 0.3530 ## colonial1 0.05380 0.04477 1.202 0.2330 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1841 on 83 degrees of freedom ## Multiple R-squared: 0.6491, Adjusted R-squared: 0.6322 ## F-statistic: 38.38 on 4 and 83 DF, p-value: &lt; 2.2e-16 the difference in log(price) between a house of colonial style and that of another style is .054 a colonial-style house is predicted to sell for about 5.4% more, holding other factors fixed when log(y) is the dependent variable in a model, the co- efficient on a dummy variable, when multiplied by 100, is interpreted as the percentage difference in y, holding all other factors fixed. ## ## Call: ## lm(formula = lwage ~ educ + female + exper + expersq + tenure + ## tenursq, data = wage1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.83160 -0.25658 -0.02126 0.25500 1.13370 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.4166910 0.0989279 4.212 2.98e-05 *** ## educ 0.0801967 0.0067573 11.868 &lt; 2e-16 *** ## female1 -0.2965110 0.0358055 -8.281 1.04e-15 *** ## exper 0.0294324 0.0049752 5.916 6.00e-09 *** ## expersq -0.0005827 0.0001073 -5.431 8.65e-08 *** ## tenure 0.0317139 0.0068452 4.633 4.56e-06 *** ## tenursq -0.0005852 0.0002347 -2.493 0.013 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3998 on 519 degrees of freedom ## Multiple R-squared: 0.4408, Adjusted R-squared: 0.4343 ## F-statistic: 68.18 on 6 and 519 DF, p-value: &lt; 2.2e-16 the coefficient on female implies that, for the same levels of educ, exper, and tenure, women earn about 100(.297) = 29.7% less than men exact percentage difference in predicted wages the difference in predicted wages between men and women is about 29.7% "],
["heteroskedasticity.html", "Chapter 8 Heteroskedasticity", " Chapter 8 Heteroskedasticity "],
["more-on-specification-and-data-issues.html", "Chapter 9 More on Specification and Data Issues", " Chapter 9 More on Specification and Data Issues "],
["basic-regression-analysis-with-time-series-data.html", "Chapter 10 Basic Regression Analysis with Time Series Data 10.1 Example 10.1 10.2 Example 10.2 10.3 Example 10.3 10.4 Example 10.4 10.5 Example 10.5 10.6 Example 10.7 10.7 Example 10.8 10.8 Example 10.9 10.9 C1 10.10 C2 10.11 C3 10.12 C4 10.13 C5 10.14 C6", " Chapter 10 Basic Regression Analysis with Time Series Data 10.1 Example 10.1 ## ## Call: ## lm(formula = inf ~ unem, data = phillips %&gt;% filter(year &lt; 1997)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.3826 -1.9797 -0.7229 1.3577 8.7562 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.4236 1.7190 0.828 0.412 ## unem 0.4676 0.2891 1.617 0.112 ## ## Residual standard error: 3.131 on 47 degrees of freedom ## Multiple R-squared: 0.05272, Adjusted R-squared: 0.03257 ## F-statistic: 2.616 on 1 and 47 DF, p-value: 0.1125 10.2 Example 10.2 ## ## Call: ## lm(formula = i3 ~ inf + def, data = intdef) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.9948 -1.1694 0.1959 0.9602 4.7224 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.73327 0.43197 4.012 0.00019 *** ## inf 0.60587 0.08213 7.376 1.12e-09 *** ## def 0.51306 0.11838 4.334 6.57e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.843 on 53 degrees of freedom ## Multiple R-squared: 0.6021, Adjusted R-squared: 0.5871 ## F-statistic: 40.09 on 2 and 53 DF, p-value: 2.483e-11 10.3 Example 10.3 ## ## Call: ## lm(formula = lprepop ~ lmincov + lusgnp, data = prminwge %&gt;% ## filter(between(year, 1950, 1987))) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.117133 -0.036998 -0.005943 0.028182 0.113938 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.05442 0.76541 -1.378 0.1771 ## lmincov -0.15444 0.06490 -2.380 0.0229 * ## lusgnp -0.01219 0.08851 -0.138 0.8913 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0557 on 35 degrees of freedom ## Multiple R-squared: 0.6605, Adjusted R-squared: 0.6411 ## F-statistic: 34.04 on 2 and 35 DF, p-value: 6.17e-09 10.4 Example 10.4 ## ## Call: ## lm(formula = gfr ~ pe + ww2 + pill, data = fertil3 %&gt;% filter(between(year, ## 1913, 1984))) ## ## Residuals: ## Min 1Q Median 3Q Max ## -27.0187 -9.6195 0.3393 9.4746 28.0730 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 98.68176 3.20813 30.760 &lt; 2e-16 *** ## pe 0.08254 0.02965 2.784 0.00694 ** ## ww2 -24.23840 7.45825 -3.250 0.00180 ** ## pill -31.59403 4.08107 -7.742 6.46e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.69 on 68 degrees of freedom ## Multiple R-squared: 0.4734, Adjusted R-squared: 0.4502 ## F-statistic: 20.38 on 3 and 68 DF, p-value: 1.575e-09 ## ## Call: ## lm(formula = gfr ~ pe + pe_1 + pe_2 + ww2 + pill, data = fertil3 %&gt;% ## filter(between(year, 1913, 1984))) ## ## Residuals: ## Min 1Q Median 3Q Max ## -24.6461 -9.5409 -0.0312 8.3378 29.1295 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 95.87050 3.28196 29.211 &lt; 2e-16 *** ## pe 0.07267 0.12553 0.579 0.5647 ## pe_1 -0.00578 0.15566 -0.037 0.9705 ## pe_2 0.03383 0.12626 0.268 0.7896 ## ww2 -22.12650 10.73197 -2.062 0.0433 * ## pill -31.30499 3.98156 -7.862 5.63e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.27 on 64 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.4986, Adjusted R-squared: 0.4594 ## F-statistic: 12.73 on 5 and 64 DF, p-value: 1.353e-08 10.5 Example 10.5 ## ## Call: ## lm(formula = lchnimp ~ lchempi + lgas + lrtwex + befile6 + affile6 + ## afdec6, data = barium) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.03356 -0.39080 0.03048 0.40248 1.51719 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.80300 21.04537 -0.846 0.3992 ## lchempi 3.11719 0.47920 6.505 1.72e-09 *** ## lgas 0.19635 0.90662 0.217 0.8289 ## lrtwex 0.98302 0.40015 2.457 0.0154 * ## befile6 0.05957 0.26097 0.228 0.8198 ## affile6 -0.03241 0.26430 -0.123 0.9026 ## afdec6 -0.56524 0.28584 -1.978 0.0502 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5974 on 124 degrees of freedom ## Multiple R-squared: 0.3049, Adjusted R-squared: 0.2712 ## F-statistic: 9.064 on 6 and 124 DF, p-value: 3.255e-08 10.6 Example 10.7 ## ## Call: ## lm(formula = linvpc ~ lprice, data = hseinv) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.45991 -0.08694 -0.01264 0.08651 0.34672 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.55023 0.04303 -12.788 1.03e-15 *** ## lprice 1.24094 0.38242 3.245 0.00238 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1554 on 40 degrees of freedom ## Multiple R-squared: 0.2084, Adjusted R-squared: 0.1886 ## F-statistic: 10.53 on 1 and 40 DF, p-value: 0.002376 ## ## Call: ## lm(formula = linvpc ~ lprice + t, data = hseinv) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.45092 -0.08583 -0.01734 0.08517 0.26024 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.913060 0.135613 -6.733 5e-08 *** ## lprice -0.380961 0.678835 -0.561 0.57787 ## t 0.009829 0.003512 2.798 0.00794 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1436 on 39 degrees of freedom ## Multiple R-squared: 0.3408, Adjusted R-squared: 0.307 ## F-statistic: 10.08 on 2 and 39 DF, p-value: 0.000296 10.7 Example 10.8 ## ## Call: ## lm(formula = gfr ~ pe + ww2 + pill + t, data = fertil3 %&gt;% filter(between(year, ## 1913, 1984))) ## ## Residuals: ## Min 1Q Median 3Q Max ## -24.6418 -8.4574 0.2445 9.3819 17.2837 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 111.76943 3.35777 33.287 &lt; 2e-16 *** ## pe 0.27888 0.04002 6.968 1.72e-09 *** ## ww2 -35.59228 6.29738 -5.652 3.53e-07 *** ## pill 0.99745 6.26163 0.159 0.874 ## t -1.14987 0.18790 -6.119 5.49e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.85 on 67 degrees of freedom ## Multiple R-squared: 0.6622, Adjusted R-squared: 0.642 ## F-statistic: 32.84 on 4 and 67 DF, p-value: 3.756e-15 ## ## Call: ## lm(formula = gfr ~ pe + ww2 + pill + t + tsq, data = fertil3 %&gt;% ## filter(between(year, 1913, 1984))) ## ## Residuals: ## Min 1Q Median 3Q Max ## -25.9791 -6.9775 -0.2713 7.7975 19.9861 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 124.091935 4.360738 28.457 &lt; 2e-16 *** ## pe 0.347813 0.040260 8.639 1.91e-12 *** ## ww2 -35.880277 5.707921 -6.286 2.95e-08 *** ## pill -10.119723 6.336094 -1.597 0.115008 ## t -2.531426 0.389386 -6.501 1.24e-08 *** ## tsq 0.019613 0.004971 3.945 0.000196 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.74 on 66 degrees of freedom ## Multiple R-squared: 0.7267, Adjusted R-squared: 0.706 ## F-statistic: 35.09 on 5 and 66 DF, p-value: &lt; 2.2e-16 10.8 Example 10.9 ## ## Call: ## lm(formula = lprepop ~ lmincov + lusgnp + t, data = prminwge %&gt;% ## filter(between(year, 1950, 1987))) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.049493 -0.024425 -0.009596 0.017055 0.086354 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -8.696298 1.295764 -6.711 1.04e-07 *** ## lmincov -0.168695 0.044246 -3.813 0.000552 *** ## lusgnp 1.057351 0.176637 5.986 8.98e-07 *** ## t -0.032354 0.005023 -6.442 2.31e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.03793 on 34 degrees of freedom ## Multiple R-squared: 0.8471, Adjusted R-squared: 0.8336 ## F-statistic: 62.78 on 3 and 34 DF, p-value: 6.007e-14 10.9 C1 ## ## Call: ## lm(formula = i3 ~ inf + def + y79, data = intdef) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.4674 -0.8407 0.2388 1.0148 3.9654 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.29623 0.42535 3.047 0.00362 ** ## inf 0.60842 0.07625 7.979 1.37e-10 *** ## def 0.36266 0.12025 3.016 0.00396 ** ## y79TRUE 1.55877 0.50577 3.082 0.00329 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.711 on 52 degrees of freedom ## Multiple R-squared: 0.6635, Adjusted R-squared: 0.6441 ## F-statistic: 34.18 on 3 and 52 DF, p-value: 2.408e-12 10.10 C2 ## ## Call: ## lm(formula = lchnimp ~ lchempi + lgas + lrtwex + befile6 + affile6 + ## afdec6, data = barium) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.03356 -0.39080 0.03048 0.40248 1.51719 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.80300 21.04537 -0.846 0.3992 ## lchempi 3.11719 0.47920 6.505 1.72e-09 *** ## lgas 0.19635 0.90662 0.217 0.8289 ## lrtwex 0.98302 0.40015 2.457 0.0154 * ## befile6 0.05957 0.26097 0.228 0.8198 ## affile6 -0.03241 0.26430 -0.123 0.9026 ## afdec6 -0.56524 0.28584 -1.978 0.0502 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5974 on 124 degrees of freedom ## Multiple R-squared: 0.3049, Adjusted R-squared: 0.2712 ## F-statistic: 9.064 on 6 and 124 DF, p-value: 3.255e-08 ## ## Call: ## lm(formula = lchnimp ~ lchempi + lgas + lrtwex + befile6 + affile6 + ## afdec6 + t, data = barium) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.94317 -0.31168 0.03172 0.36366 1.21218 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.367526 20.782165 -0.114 0.90949 ## lchempi -0.686236 1.239711 -0.554 0.58089 ## lgas 0.465679 0.876178 0.531 0.59604 ## lrtwex 0.078224 0.472440 0.166 0.86876 ## befile6 0.090470 0.251289 0.360 0.71945 ## affile6 0.097006 0.257313 0.377 0.70683 ## afdec6 -0.351502 0.282542 -1.244 0.21584 ## t 0.012706 0.003844 3.305 0.00124 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5748 on 123 degrees of freedom ## Multiple R-squared: 0.3616, Adjusted R-squared: 0.3252 ## F-statistic: 9.951 on 7 and 123 DF, p-value: 8.358e-10 ## ## Call: ## lm(formula = lchnimp ~ lchempi + lgas + lrtwex + befile6 + affile6 + ## afdec6 + t + feb + mar + apr + may + jun + jul + aug + sep + ## oct + nov + dec, data = barium) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.86054 -0.36284 0.02233 0.37155 1.09845 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.300074 31.397067 0.870 0.38643 ## lchempi -0.451656 1.271528 -0.355 0.72310 ## lgas -0.820624 1.345056 -0.610 0.54303 ## lrtwex -0.197141 0.529531 -0.372 0.71038 ## befile6 0.164851 0.256979 0.641 0.52251 ## affile6 0.153400 0.271986 0.564 0.57388 ## afdec6 -0.295016 0.299428 -0.985 0.32662 ## t 0.012339 0.003916 3.151 0.00209 ** ## feb -0.355415 0.293754 -1.210 0.22886 ## mar 0.062566 0.254858 0.245 0.80652 ## apr -0.440615 0.258398 -1.705 0.09093 . ## may 0.031299 0.259200 0.121 0.90410 ## jun -0.200950 0.259213 -0.775 0.43984 ## jul 0.011111 0.268378 0.041 0.96705 ## aug -0.127114 0.267792 -0.475 0.63594 ## sep -0.075193 0.258350 -0.291 0.77155 ## oct 0.079763 0.257051 0.310 0.75691 ## nov -0.260303 0.253062 -1.029 0.30588 ## dec 0.096533 0.261553 0.369 0.71277 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5788 on 112 degrees of freedom ## Multiple R-squared: 0.4106, Adjusted R-squared: 0.3158 ## F-statistic: 4.334 on 18 and 112 DF, p-value: 6.188e-07 10.11 C3 ## ## Call: ## lm(formula = lprepop ~ lmincov + lusgnp + lprgnp + t, data = prminwge %&gt;% ## filter(between(year, 1950, 1987))) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.054679 -0.023653 -0.004039 0.018638 0.076947 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.663432 1.257831 -5.298 7.67e-06 *** ## lmincov -0.212261 0.040152 -5.286 7.92e-06 *** ## lusgnp 0.486046 0.221983 2.190 0.0357 * ## lprgnp 0.285239 0.080492 3.544 0.0012 ** ## t -0.026663 0.004627 -5.763 1.94e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.03277 on 33 degrees of freedom ## Multiple R-squared: 0.8892, Adjusted R-squared: 0.8758 ## F-statistic: 66.23 on 4 and 33 DF, p-value: 2.677e-15 10.12 C4 10.13 C5 10.14 C6 ## ## Call: ## lm(formula = gfr ~ t + tsq, data = fertil3 %&gt;% filter(between(year, ## 1913, 1984))) ## ## Residuals: ## Min 1Q Median 3Q Max ## -28.7519 -12.5333 0.3168 13.7611 28.7346 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 107.056263 6.049651 17.696 &lt;2e-16 *** ## t 0.071697 0.382446 0.187 0.852 ## tsq -0.007959 0.005077 -1.568 0.122 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 16.64 on 69 degrees of freedom ## Multiple R-squared: 0.3141, Adjusted R-squared: 0.2942 ## F-statistic: 15.8 on 2 and 69 DF, p-value: 2.243e-06 "],
["further-issues-in-using-ols-with-time-series-data.html", "Chapter 11 Further Issues in Using OLS with Time Series Data", " Chapter 11 Further Issues in Using OLS with Time Series Data "],
["serial-correlation-and-heteroskedasticity-in-time-series-regressions.html", "Chapter 12 Serial Correlation and Heteroskedasticity in Time Series Regressions", " Chapter 12 Serial Correlation and Heteroskedasticity in Time Series Regressions "],
["pooling-cross-sections-across-time-simple-panel-data-method.html", "Chapter 13 Pooling Cross Sections Across Time: Simple Panel Data Method 13.1 Exercise 13.1 13.2 Exercise 13.2 13.3 Exercise 13.2 13.4 Exercise 13.3", " Chapter 13 Pooling Cross Sections Across Time: Simple Panel Data Method 13.1 Exercise 13.1 data(&#39;fertil1&#39;) fertil1 &lt;- fertil1 %&gt;% select(educ, age:agesq) fit &lt;- lm(kids~., data=fertil1) summary(fit) ## ## Call: ## lm(formula = kids ~ ., data = fertil1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.9878 -1.0086 -0.0767 0.9331 4.6548 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.742457 3.051767 -2.537 0.011315 * ## educ -0.128427 0.018349 -6.999 4.44e-12 *** ## age 0.532135 0.138386 3.845 0.000127 *** ## black 1.075658 0.173536 6.198 8.02e-10 *** ## east 0.217324 0.132788 1.637 0.101992 ## northcen 0.363114 0.120897 3.004 0.002729 ** ## west 0.197603 0.166913 1.184 0.236719 ## farm -0.052557 0.147190 -0.357 0.721105 ## othrural -0.162854 0.175442 -0.928 0.353481 ## town 0.084353 0.124531 0.677 0.498314 ## smcity 0.211879 0.160296 1.322 0.186507 ## y74 0.268183 0.172716 1.553 0.120771 ## y76 -0.097379 0.179046 -0.544 0.586633 ## y78 -0.068666 0.181684 -0.378 0.705544 ## y80 -0.071305 0.182771 -0.390 0.696511 ## y82 -0.522484 0.172436 -3.030 0.002502 ** ## y84 -0.545166 0.174516 -3.124 0.001831 ** ## agesq -0.005804 0.001564 -3.710 0.000217 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.555 on 1111 degrees of freedom ## Multiple R-squared: 0.1295, Adjusted R-squared: 0.1162 ## F-statistic: 9.723 on 17 and 1111 DF, p-value: &lt; 2.2e-16 13.2 Exercise 13.2 data(&#39;cps78_85&#39;) fit &lt;- lm(lwage~y85+educ+y85educ+exper+expersq+union+female+y85fem, data=cps78_85) summary(fit) ## ## Call: ## lm(formula = lwage ~ y85 + educ + y85educ + exper + expersq + ## union + female + y85fem, data = cps78_85) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.56098 -0.25828 0.00864 0.26571 2.11669 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.589e-01 9.345e-02 4.911 1.05e-06 *** ## y85 1.178e-01 1.238e-01 0.952 0.3415 ## educ 7.472e-02 6.676e-03 11.192 &lt; 2e-16 *** ## y85educ 1.846e-02 9.354e-03 1.974 0.0487 * ## exper 2.958e-02 3.567e-03 8.293 3.27e-16 *** ## expersq -3.994e-04 7.754e-05 -5.151 3.08e-07 *** ## union 2.021e-01 3.029e-02 6.672 4.03e-11 *** ## female -3.167e-01 3.662e-02 -8.648 &lt; 2e-16 *** ## y85fem 8.505e-02 5.131e-02 1.658 0.0977 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4127 on 1075 degrees of freedom ## Multiple R-squared: 0.4262, Adjusted R-squared: 0.4219 ## F-statistic: 99.8 on 8 and 1075 DF, p-value: &lt; 2.2e-16 13.3 Exercise 13.2 data(&#39;kielmc&#39;) fit &lt;- lm(rprice~nearinc, data=kielmc %&gt;% filter(year == 1981)) summary(fit) ## ## Call: ## lm(formula = rprice ~ nearinc, data = kielmc %&gt;% filter(year == ## 1981)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -60678 -19832 -2997 21139 136754 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 101308 3093 32.754 &lt; 2e-16 *** ## nearinc -30688 5828 -5.266 5.14e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 31240 on 140 degrees of freedom ## Multiple R-squared: 0.1653, Adjusted R-squared: 0.1594 ## F-statistic: 27.73 on 1 and 140 DF, p-value: 5.139e-07 fit &lt;- lm(rprice~nearinc, data=kielmc %&gt;% filter(year == 1978)) summary(fit) ## ## Call: ## lm(formula = rprice ~ nearinc, data = kielmc %&gt;% filter(year == ## 1978)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -56517 -16605 -3193 8683 236307 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 82517 2654 31.094 &lt; 2e-16 *** ## nearinc -18824 4745 -3.968 0.000105 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 29430 on 177 degrees of freedom ## Multiple R-squared: 0.08167, Adjusted R-squared: 0.07648 ## F-statistic: 15.74 on 1 and 177 DF, p-value: 0.0001054 fit &lt;- lm(rprice~y81+nearinc+y81nrinc, data=kielmc %&gt;% filter(between(year, 1978, 1981))) summary(fit) ## ## Call: ## lm(formula = rprice ~ y81 + nearinc + y81nrinc, data = kielmc %&gt;% ## filter(between(year, 1978, 1981))) ## ## Residuals: ## Min 1Q Median 3Q Max ## -60678 -17693 -3031 12483 236307 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 82517 2727 30.260 &lt; 2e-16 *** ## y81 18790 4050 4.640 5.12e-06 *** ## nearinc -18824 4875 -3.861 0.000137 *** ## y81nrinc -11864 7457 -1.591 0.112595 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 30240 on 317 degrees of freedom ## Multiple R-squared: 0.1739, Adjusted R-squared: 0.1661 ## F-statistic: 22.25 on 3 and 317 DF, p-value: 4.224e-13 13.4 Exercise 13.3 data(&#39;injury&#39;) fit &lt;- lm(ldurat~afchnge+highearn+I(afchnge*highearn), data=injury) summary(fit) ## ## Call: ## lm(formula = ldurat ~ afchnge + highearn + I(afchnge * highearn), ## data = injury) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.0128 -0.7214 -0.0171 0.7714 4.0047 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.19934 0.02711 44.241 &lt; 2e-16 *** ## afchnge 0.02364 0.03970 0.595 0.55164 ## highearn 0.21520 0.04336 4.963 7.11e-07 *** ## I(afchnge * highearn) 0.18835 0.06279 2.999 0.00271 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.298 on 7146 degrees of freedom ## Multiple R-squared: 0.01584, Adjusted R-squared: 0.01543 ## F-statistic: 38.34 on 3 and 7146 DF, p-value: &lt; 2.2e-16 "]
]
